{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stack_overflow_tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ihs1Hb3RU93S",
        "colab": {},
        "outputId": "6dbb8bb9-e287-4d49-f2ea-2df86731d425"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "from flask_sqlalchemy import SQLAlchemy\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import sqlalchemy\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from skmultilearn.adapt import mlknn\n",
        "#from skmultilearn.problem_transform import ClassifierChain\n",
        "#from skmultilearn.problem_transform import BinaryRelevance\n",
        "#from skmultilearn.problem_transform import LabelPowerset\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime\n",
        "print('Done importing all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done importing all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elwijxMGU93w"
      },
      "source": [
        "<h3> 2.1.1 Data Overview </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mdFiIj7_U93x"
      },
      "source": [
        "Refer: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data\n",
        "<br>\n",
        "All of the data is in 2 files: Train and Test.<br />\n",
        "<pre>\n",
        "<b>Train.csv</b> contains 4 columns: Id,Title,Body,Tags.<br />\n",
        "<b>Test.csv</b> contains the same columns but without the Tags, which you are to predict.<br />\n",
        "<b>Size of Train.csv</b> - 6.75GB<br />\n",
        "<b>Size of Test.csv</b> - 2GB<br />\n",
        "<b>Number of rows in Train.csv</b> = 6034195<br />\n",
        "</pre>\n",
        "The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).<br />\n",
        "<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ji0A66hWU93z"
      },
      "source": [
        "__Data Field Explaination__\n",
        "\n",
        "Dataset contains 6,034,195 rows. The columns in the table are:<br />\n",
        "<pre>\n",
        "<b>Id</b> - Unique identifier for each question<br />\n",
        "<b>Title</b> - The question's title<br />\n",
        "<b>Body</b> - The body of the question<br />\n",
        "<b>Tags</b> - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\\t' or ampersands '&')<br />\n",
        "</pre>\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WNDiy42GU931"
      },
      "source": [
        "<h3>2.1.2 Example Data point </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "D5IcxRbYU932"
      },
      "source": [
        "<pre>\n",
        "<b>Title</b>:  Implementing Boundary Value Analysis of Software Testing in a C++ program?\n",
        "<b>Body </b>: <pre><code>\n",
        "        #include&lt;\n",
        "        iostream&gt;\\n\n",
        "        #include&lt;\n",
        "        stdlib.h&gt;\\n\\n\n",
        "        using namespace std;\\n\\n\n",
        "        int main()\\n\n",
        "        {\\n\n",
        "                 int n,a[n],x,c,u[n],m[n],e[n][4];\\n         \n",
        "                 cout&lt;&lt;\"Enter the number of variables\";\\n         cin&gt;&gt;n;\\n\\n         \n",
        "                 cout&lt;&lt;\"Enter the Lower, and Upper Limits of the variables\";\\n         \n",
        "                 for(int y=1; y&lt;n+1; y++)\\n         \n",
        "                 {\\n                 \n",
        "                    cin&gt;&gt;m[y];\\n                 \n",
        "                    cin&gt;&gt;u[y];\\n         \n",
        "                 }\\n         \n",
        "                 for(x=1; x&lt;n+1; x++)\\n         \n",
        "                 {\\n                 \n",
        "                    a[x] = (m[x] + u[x])/2;\\n         \n",
        "                 }\\n         \n",
        "                 c=(n*4)-4;\\n         \n",
        "                 for(int a1=1; a1&lt;n+1; a1++)\\n         \n",
        "                 {\\n\\n             \n",
        "                    e[a1][0] = m[a1];\\n             \n",
        "                    e[a1][1] = m[a1]+1;\\n             \n",
        "                    e[a1][2] = u[a1]-1;\\n             \n",
        "                    e[a1][3] = u[a1];\\n         \n",
        "                 }\\n         \n",
        "                 for(int i=1; i&lt;n+1; i++)\\n         \n",
        "                 {\\n            \n",
        "                    for(int l=1; l&lt;=i; l++)\\n            \n",
        "                    {\\n                 \n",
        "                        if(l!=1)\\n                 \n",
        "                        {\\n                    \n",
        "                            cout&lt;&lt;a[l]&lt;&lt;\"\\\\t\";\\n                 \n",
        "                        }\\n            \n",
        "                    }\\n            \n",
        "                    for(int j=0; j&lt;4; j++)\\n            \n",
        "                    {\\n                \n",
        "                        cout&lt;&lt;e[i][j];\\n                \n",
        "                        for(int k=0; k&lt;n-(i+1); k++)\\n                \n",
        "                        {\\n                    \n",
        "                            cout&lt;&lt;a[k]&lt;&lt;\"\\\\t\";\\n               \n",
        "                        }\\n                \n",
        "                        cout&lt;&lt;\"\\\\n\";\\n            \n",
        "                    }\\n        \n",
        "                 }    \\n\\n        \n",
        "                 system(\"PAUSE\");\\n        \n",
        "                 return 0;    \\n\n",
        "        }\\n\n",
        "        </code></pre>\\n\\n\n",
        "        <p>The answer should come in the form of a table like</p>\\n\\n\n",
        "        <pre><code>       \n",
        "        1            50              50\\n       \n",
        "        2            50              50\\n       \n",
        "        99           50              50\\n       \n",
        "        100          50              50\\n       \n",
        "        50           1               50\\n       \n",
        "        50           2               50\\n       \n",
        "        50           99              50\\n       \n",
        "        50           100             50\\n       \n",
        "        50           50              1\\n       \n",
        "        50           50              2\\n       \n",
        "        50           50              99\\n       \n",
        "        50           50              100\\n\n",
        "        </code></pre>\\n\\n\n",
        "        <p>if the no of inputs is 3 and their ranges are\\n\n",
        "        1,100\\n\n",
        "        1,100\\n\n",
        "        1,100\\n\n",
        "        (could be varied too)</p>\\n\\n\n",
        "        <p>The output is not coming,can anyone correct the code or tell me what\\'s wrong?</p>\\n'\n",
        "<b>Tags </b>: 'c++ c'\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MomJjFPnU934"
      },
      "source": [
        "<h2>2.2 Mapping the real-world problem to a Machine Learning Problem </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJ5lyxIUU936"
      },
      "source": [
        "<h3> 2.2.1 Type of Machine Learning Problem </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Jb9u-38U938"
      },
      "source": [
        "<p> It is a multi-label classification problem  <br>\n",
        "<b>Multi-label Classification</b>: Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stackoverflow might be about any of C, Pointers, FileIO and/or memory-management at the same time or none of these. <br>\n",
        "__Credit__: http://scikit-learn.org/stable/modules/multiclass.html\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QIOwycKkU93_"
      },
      "source": [
        "<h3>2.2.2 Performance metric </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DDQXZ4k5U94A"
      },
      "source": [
        "<b>Micro-Averaged F1-Score (Mean F Score) </b>: \n",
        "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "\n",
        "<i>F1 = 2 * (precision * recall) / (precision + recall)</i><br>\n",
        "\n",
        "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class. <br>\n",
        "\n",
        "<b>'Micro f1 score': </b><br>\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
        "<br>\n",
        "\n",
        "<b>'Macro f1 score': </b><br>\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "<br>\n",
        "\n",
        "https://www.kaggle.com/wiki/MeanFScore <br>\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html <br>\n",
        "<br>\n",
        "<b> Hamming loss </b>: The Hamming loss is the fraction of labels that are incorrectly predicted. <br>\n",
        "https://www.kaggle.com/wiki/HammingLoss <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xqsVRjSmU94C"
      },
      "source": [
        "<h1> 3. Exploratory Data Analysis </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azda-BmRU94H"
      },
      "source": [
        "<h2> 3.1 Data Loading and Cleaning </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62GDC_VjU94J"
      },
      "source": [
        "<h3>3.1.1 Using Pandas with SQLite to Load the data</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEAYcGi_ML_K",
        "colab_type": "code",
        "colab": {},
        "outputId": "9c96456d-5119-4034-87f1-c8f3cc9931c4"
      },
      "source": [
        "#Creating db file from csv\n",
        "start = datetime.now()\n",
        "disk_engine = create_engine('sqlite:///train.db')\n",
        "\n",
        "start = dt.datetime.now()\n",
        "chunksize = 100000\n",
        "j = 0\n",
        "index_start = 1\n",
        "for df in pd.read_csv('Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
        "    df.index += index_start\n",
        "    j+=1\n",
        "    print('{} rows'.format(j*chunksize))\n",
        "    df.to_sql('train_data_of_stackoverflow', disk_engine, if_exists='append')\n",
        "    index_start = df.index[-1] + 1\n",
        "    print(\"Time taken to run this cell :\", datetime.now() - start)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 rows\n",
            "Time taken to run this cell : 0:00:16.814435\n",
            "200000 rows\n",
            "Time taken to run this cell : 0:00:24.448987\n",
            "300000 rows\n",
            "Time taken to run this cell : 0:00:32.435646\n",
            "400000 rows\n",
            "Time taken to run this cell : 0:00:40.369646\n",
            "500000 rows\n",
            "Time taken to run this cell : 0:00:48.488472\n",
            "600000 rows\n",
            "Time taken to run this cell : 0:00:56.467731\n",
            "700000 rows\n",
            "Time taken to run this cell : 0:01:04.549541\n",
            "800000 rows\n",
            "Time taken to run this cell : 0:01:12.471692\n",
            "900000 rows\n",
            "Time taken to run this cell : 0:01:20.393455\n",
            "1000000 rows\n",
            "Time taken to run this cell : 0:01:28.205321\n",
            "1100000 rows\n",
            "Time taken to run this cell : 0:01:35.980612\n",
            "1200000 rows\n",
            "Time taken to run this cell : 0:01:43.631223\n",
            "1300000 rows\n",
            "Time taken to run this cell : 0:01:51.895985\n",
            "1400000 rows\n",
            "Time taken to run this cell : 0:01:59.021229\n",
            "1500000 rows\n",
            "Time taken to run this cell : 0:02:06.206323\n",
            "1600000 rows\n",
            "Time taken to run this cell : 0:02:13.053242\n",
            "1700000 rows\n",
            "Time taken to run this cell : 0:02:19.806309\n",
            "1800000 rows\n",
            "Time taken to run this cell : 0:02:26.529283\n",
            "1900000 rows\n",
            "Time taken to run this cell : 0:02:33.389793\n",
            "2000000 rows\n",
            "Time taken to run this cell : 0:02:40.286096\n",
            "2100000 rows\n",
            "Time taken to run this cell : 0:02:47.484527\n",
            "2200000 rows\n",
            "Time taken to run this cell : 0:02:54.515509\n",
            "2300000 rows\n",
            "Time taken to run this cell : 0:03:02.389808\n",
            "2400000 rows\n",
            "Time taken to run this cell : 0:03:09.348553\n",
            "2500000 rows\n",
            "Time taken to run this cell : 0:03:16.504127\n",
            "2600000 rows\n",
            "Time taken to run this cell : 0:03:23.473060\n",
            "2700000 rows\n",
            "Time taken to run this cell : 0:03:30.495699\n",
            "2800000 rows\n",
            "Time taken to run this cell : 0:03:37.421675\n",
            "2900000 rows\n",
            "Time taken to run this cell : 0:03:44.619511\n",
            "3000000 rows\n",
            "Time taken to run this cell : 0:03:51.736209\n",
            "3100000 rows\n",
            "Time taken to run this cell : 0:03:59.100239\n",
            "3200000 rows\n",
            "Time taken to run this cell : 0:04:06.150052\n",
            "3300000 rows\n",
            "Time taken to run this cell : 0:04:13.417716\n",
            "3400000 rows\n",
            "Time taken to run this cell : 0:04:20.678650\n",
            "3500000 rows\n",
            "Time taken to run this cell : 0:04:28.218265\n",
            "3600000 rows\n",
            "Time taken to run this cell : 0:04:35.977438\n",
            "3700000 rows\n",
            "Time taken to run this cell : 0:04:44.146600\n",
            "3800000 rows\n",
            "Time taken to run this cell : 0:04:51.271802\n",
            "3900000 rows\n",
            "Time taken to run this cell : 0:04:58.699664\n",
            "4000000 rows\n",
            "Time taken to run this cell : 0:05:05.817907\n",
            "4100000 rows\n",
            "Time taken to run this cell : 0:05:12.953025\n",
            "4200000 rows\n",
            "Time taken to run this cell : 0:05:20.199818\n",
            "4300000 rows\n",
            "Time taken to run this cell : 0:05:27.687480\n",
            "4400000 rows\n",
            "Time taken to run this cell : 0:05:34.998835\n",
            "4500000 rows\n",
            "Time taken to run this cell : 0:05:42.185017\n",
            "4600000 rows\n",
            "Time taken to run this cell : 0:05:49.634094\n",
            "4700000 rows\n",
            "Time taken to run this cell : 0:05:57.294978\n",
            "4800000 rows\n",
            "Time taken to run this cell : 0:06:05.639677\n",
            "4900000 rows\n",
            "Time taken to run this cell : 0:06:13.564874\n",
            "5000000 rows\n",
            "Time taken to run this cell : 0:06:21.073485\n",
            "5100000 rows\n",
            "Time taken to run this cell : 0:06:28.415967\n",
            "5200000 rows\n",
            "Time taken to run this cell : 0:06:36.274012\n",
            "5300000 rows\n",
            "Time taken to run this cell : 0:06:44.149100\n",
            "5400000 rows\n",
            "Time taken to run this cell : 0:06:51.557722\n",
            "5500000 rows\n",
            "Time taken to run this cell : 0:06:58.869096\n",
            "5600000 rows\n",
            "Time taken to run this cell : 0:07:06.003746\n",
            "5700000 rows\n",
            "Time taken to run this cell : 0:07:13.543783\n",
            "5800000 rows\n",
            "Time taken to run this cell : 0:07:20.853520\n",
            "5900000 rows\n",
            "Time taken to run this cell : 0:07:29.640555\n",
            "6000000 rows\n",
            "Time taken to run this cell : 0:07:36.477392\n",
            "6100000 rows\n",
            "Time taken to run this cell : 0:07:39.232582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d5yUhXVNU94Q"
      },
      "source": [
        "<h3> 3.1.2 Counting the number of rows </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ORCclXYU94R",
        "outputId": "9625d8e0-bf34-413e-8246-932fa1cb21b7",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "#*************** Now we have a sqlite database, every time when we have to access it, just use the 'connect' command**************\n",
        "\n",
        "con = sqlite3.connect('train.db')\n",
        "num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM train_data_of_stackoverflow\"\"\", con)\n",
        "#Always remember to close the database\n",
        "\n",
        "\n",
        "print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n",
        "con.close()\n",
        "print(\"Time taken to count the number of rows :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the database : \n",
            " 36205176\n",
            "Time taken to count the number of rows : 0:00:07.908222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xso2eOEvU94Z"
      },
      "source": [
        "<h3>3.1.3 Checking for duplicates </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBHCcr3DU94b",
        "outputId": "2340d414-8570-4d7e-fda6-a69c39f13780",
        "colab": {}
      },
      "source": [
        "#Learn SQl: https://www.w3schools.com/sql/default.asp\n",
        "# if os.path.isfile('train.db'):\n",
        "start = datetime.now()\n",
        "con = sqlite3.connect('train.db')\n",
        "df_no_dup = pd.read_sql('SELECT Title, Body, Tags, COUNT(*) as Count_duplicate_questions FROM train_data_of_stackoverflow GROUP BY Title, Body, Tags', con)\n",
        "con.close()\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql 'SELECT Title, Body, Tags, COUNT(*) as Count_duplicate_questions FROM train_data_of_stackoverflow GROUP BY Title, Body, Tags': database or disk is full",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1594\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mOperationalError\u001b[0m: database or disk is full",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-4-44509b173949>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_no_dup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SELECT Title, Body, Tags, COUNT(*) as Count_duplicate_questions FROM train_data_of_stackoverflow GROUP BY Title, Body, Tags'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time taken to run this cell :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         )\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1608\u001b[0m                 \u001b[1;34m\"Execution failed on sql '{sql}': {exc}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m             )\n\u001b[1;32m-> 1610\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1593\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT Title, Body, Tags, COUNT(*) as Count_duplicate_questions FROM train_data_of_stackoverflow GROUP BY Title, Body, Tags': database or disk is full"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gap4NRPWU94h",
        "colab": {}
      },
      "source": [
        "df_no_dup.head()\n",
        "# we can observe that there are duplicates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JzFO4EeDU94n",
        "colab": {}
      },
      "source": [
        "print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0], \n",
        "      \"(\",(1-((df_no_dup.shape[0])/(num_rows['count(*)'].values[0])))*100,\"% )\")\n",
        "\n",
        "\n",
        "# From the 6 million  ,1.8 million are duplicates "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gd2VdpN6U94t",
        "colab": {}
      },
      "source": [
        "# number of times each question appeared in our database\n",
        "df_no_dup.Count_duplicate_questions.value_counts()\n",
        "\n",
        "\n",
        "\n",
        "# only 6 questions that are appear 5 times\n",
        "# questions that appear 1 times are -> 2.6 millions  ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHY-JX4sMMAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df_no_dup\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EogeNAhCU94z",
        "colab": {}
      },
      "source": [
        "sd=[]\n",
        "start = datetime.now()\n",
        "for i in range(df_no_dup.shape[0]):\n",
        "    f=df_no_dup[\"Tags\"][i]# no of characters==0 \n",
        "    if f==None:# when no tag given just remove that datapoint\n",
        "        df_no_dup=df_no_dup.drop(i,axis=0)     # remove this datapoint\n",
        "    else:\n",
        "        d=len(df_no_dup[\"Tags\"][i].split(\" \"))\n",
        "        sd.append(d)\n",
        "        \n",
        "print(datetime.now()-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StVpANHsMMBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_no_dup.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkPdkQt_MMBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_no_dup[\"Tag_Count\"] = df_no_dup[\"Tags\"].apply(lambda text: len(text.split(\" \")))\n",
        "# adding a new feature number of tags per question\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "df_no_dup.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iItMHo6MU948",
        "colab": {}
      },
      "source": [
        "# distribution of number of tags per question\n",
        "df_no_dup.Tag_Count.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wvf_wAQMMCC",
        "colab_type": "text"
      },
      "source": [
        "<h3> Save the Non_duplicate questions in a new database </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2xMgCGUKU95C",
        "colab": {}
      },
      "source": [
        "#Creating a new database with no duplicates\n",
        "if not os.path.isfile('train_no_dup.db'):\n",
        "    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n",
        "    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n",
        "    no_dup.to_sql('no_dup_train',disk_dup)\n",
        "    \n",
        "    \n",
        "    \n",
        "#***********************************************train_no_dup.db is the new database **************************************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Ou53MzeU95H",
        "colab": {}
      },
      "source": [
        "#This method seems more appropriate to work with this much data.\n",
        "#creating the connection with database file.\n",
        "#if os.path.isfile('train_no_dup.db'):\n",
        "start = datetime.now()\n",
        "con = sqlite3.connect('train_no_dup.db')\n",
        "tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n",
        "    #Always remember to close the database\n",
        "con.close()\n",
        "\n",
        "    # Let's now drop unwanted column.\n",
        "tag_data.drop(tag_data.index[0], inplace=True)\n",
        "    #Printing first 5 columns from our data frame\n",
        "tag_data.head()\n",
        "print(\" The Time taken to run this cell is :\", datetime.now() - start)\n",
        "# else:\n",
        "#     print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6q9s1SQMMCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_data.head()\n",
        "#no_dup.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hwZVL3doU95O"
      },
      "source": [
        "<h2> 3.2 Analysis of Tags </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zYs5peW8U95P"
      },
      "source": [
        "<h3> 3.2.1 Total number of unique tags </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROdC95M_U95Q",
        "colab": {}
      },
      "source": [
        "#****************************************First we have to count (A tag appear how many times)  or  frequency of tags.\n",
        "# this can be done by countvectorizer that can give us Tag_name : Frequency\n",
        "# Importing & Initializing the \"CountVectorizer\" object, which \n",
        "#is scikit-learn's bag of words tool.\n",
        "\n",
        "#by default 'split()' will tokenize each tag using space.\n",
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of strings.\n",
        "tag_dtm = vectorizer.fit_transform(tag_data['Tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oz5N0GH0U95V",
        "colab": {}
      },
      "source": [
        "print(\"Number of data points :\", tag_dtm.shape[0])\n",
        "print(\"Number of unique tags :\", tag_dtm.shape[1])\n",
        "# we have  42048 total  unique tags! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Otn6CUuQU95b",
        "colab": {}
      },
      "source": [
        "#'get_feature_name()' gives us the vocabulary.\n",
        "tags = vectorizer.get_feature_names()\n",
        "#Lets look at the tags we have.\n",
        "print(\"Some of the tags we have :\", tags[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NQa3ETSeU95g"
      },
      "source": [
        "<h3> 3.2.3 Number of times a tag appeared </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "beThyuyqU95h",
        "colab": {}
      },
      "source": [
        "#    THIS IS THE REPRESENTATION OF THE DATAPOINTS WITH THEIR DIMENSIONS       (SPARCE MATRIX)\n",
        "\n",
        "'''         TAG1    TAG2     TAG3     .   ..  ..    TAG42048\n",
        "DP1      1          0               1                            0\n",
        "DP2      0          0                1                            1\n",
        "DP3       0         0                0                            1\n",
        ".\n",
        ".\n",
        "DP4206307   0                   1                           1\n",
        "\n",
        "\n",
        "\n",
        "for calculating how many times a single tag appeared, we have to count the number of one's in each column\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements\n",
        "#Lets now store the document term matrix in a dictionary.\n",
        "\n",
        "\n",
        "'''Each row in the array is one of your original documents (strings), each column is a feature (word),\n",
        "and the element is the count for that particular word and document.\n",
        "You can see that if you sum each column you'll get the correct number'''\n",
        "freqs = tag_dtm.sum(axis=0).A1\n",
        "result = dict(zip(tags, freqs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ALwSSx9U95m",
        "colab": {}
      },
      "source": [
        "#************************************Saving this dictionary  of tagsto csv files **********************************************\n",
        "\n",
        "if not os.path.isfile('tag_counts_dict_dtm.csv'):\n",
        "    with open('tag_counts_dict_dtm.csv', 'w') as csv_file:#   parameter is 'w' this means we are writing the file in the harddisk\n",
        "        writer = csv.writer(csv_file)\n",
        "        for key, value in result.items():\n",
        "            writer.writerow([key, value])\n",
        "tag_df = pd.read_csv(\"tag_counts_dict_dtm.csv\", names=['Tags', 'Counts'])\n",
        "tag_df.head()\n",
        "\n",
        "\n",
        "\n",
        "# We are saving each and every thing to database or file, so that if our computer crashes we can start from their-> where we left   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mzmS8yiNU95t",
        "colab": {}
      },
      "source": [
        "# **************************Sort the tags in DESC order, so that we can find the most frequent tags*******************************\n",
        "\n",
        "\n",
        "\n",
        "tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n",
        "tag_counts = tag_df_sorted['Counts'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6mcj55FIU95z",
        "colab": {}
      },
      "source": [
        "plt.plot(tag_counts)\n",
        "plt.title(\"Distribution of number of times tag appeared questions\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UzTqln6XU955",
        "colab": {}
      },
      "source": [
        "# first 10k tags\n",
        "\n",
        "plt.plot(tag_counts[0:10000])\n",
        "plt.title('first 10k tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])#  :25 is the step sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT-DWs9CMME7",
        "colab_type": "text"
      },
      "source": [
        "<h2>Observations:</h2>\n",
        " -  Some Tags appear zero times,but its not much clear how many tags appear zero times, we have to zoom the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ntm8E_K9U95-",
        "colab": {}
      },
      "source": [
        "plt.plot(tag_counts[0:1000])\n",
        "plt.title('first 1k tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])         # these are the step sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-SRUeKuWU96I",
        "colab": {}
      },
      "source": [
        "plt.plot(tag_counts[0:500])\n",
        "plt.title('first 500 tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:500:5]), tag_counts[0:500:5])\n",
        "\n",
        "\n",
        "\n",
        "# some tags are very huge in number  , some tags are very less in number."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68G--G9MMFi",
        "colab_type": "text"
      },
      "source": [
        "<h2>Observations:</h2>\n",
        "-  Some Tags appear large number of  times and some tags are appear very few times, so we can say micro average f1 is good matric for\n",
        "\n",
        "   measuring performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CCOE0Nu4U96N",
        "colab": {}
      },
      "source": [
        "plt.plot(tag_counts[0:100], c='b')\n",
        "plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
        "#quantiles with 0.25 difference\n",
        "plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n",
        "\n",
        "plt.title('first 100 tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(len(tag_counts[0:100:5]), tag_counts[0:100:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnwXytypU96R",
        "colab": {}
      },
      "source": [
        "# Store tags greater than 10K in one list\n",
        "lst_tags_gt_10k = tag_df[tag_df.Counts>10000]\n",
        "#Print the length of the list\n",
        "print ('{} Tags are used more than 10000 times'.format(len(lst_tags_gt_10k)))\n",
        "# Store tags greater than 100K in one list\n",
        "lst_tags_gt_100k = tag_df[tag_df.Counts>100000]\n",
        "#Print the length of the list.\n",
        "print ('{} Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oOXPF7q1U96W"
      },
      "source": [
        "<b>Observations:</b><br />\n",
        "1. There are total 153 tags which are used more than 10000 times.\n",
        "2. 14 tags are used more than 100000 times.\n",
        "3. Most frequent tag (i.e. c#) is used 331505 times.\n",
        "4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zb-Tg_sgU96a"
      },
      "source": [
        "<h3> 3.2.4 Tags Per Question </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bsPStbjGU96g",
        "colab": {}
      },
      "source": [
        "#    THIS IS THE REPRESENTATION OF THE DATAPOINTS WITH THEIR DIMENSIONS       (SPARCE MATRIX)\n",
        "\n",
        "'''         TAG1    TAG2     TAG3     .   ..  ..    TAG42048\n",
        "DP1      1          0               1                            0\n",
        "DP2      0          0                1                            1\n",
        "DP3       0         0                0                            1\n",
        ".\n",
        ".\n",
        "DP4206307   0                   1                           1\n",
        "\n",
        "\n",
        "\n",
        "for calculating in one questions how many tags apear, just sum  the numer of ones in the single row.\n",
        "'''\n",
        "\n",
        "#Storing the count of tag in each question in list 'tag_count'\n",
        "tag_quest_count = tag_dtm.sum(axis=1).tolist()\n",
        "\n",
        "#Converting list of lists into single list, we will get [[3], [4], [2], [2], [3]] and we are converting this to [3, 4, 2, 2, 3]\n",
        "tag_quest_count=[int(j) for i in tag_quest_count for j in i]\n",
        "print ('We have total {} datapoints.'.format(len(tag_quest_count)))\n",
        "\n",
        "print(tag_quest_count[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DLbm7crfU96n",
        "colab": {}
      },
      "source": [
        "print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\n",
        "print( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\n",
        "print( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mb1vdd8KU96x",
        "colab": {}
      },
      "source": [
        "sns.countplot(tag_quest_count, palette='gist_rainbow')\n",
        "plt.title(\"Number of tags in the questions \")\n",
        "plt.xlabel(\"Number of Tags\")\n",
        "plt.ylabel(\"Number of questions\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0RsUcQkNU963"
      },
      "source": [
        "<b>Observations:</b><br />\n",
        "1. Maximum number of tags per question: 5\n",
        "2. Minimum number of tags per question: 1\n",
        "3. Avg. number of tags per question: 2.899\n",
        "4. Most of the questions are having 2 or 3 tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-M5-A-MU963"
      },
      "source": [
        "<h3>3.2.5 Most Frequent Tags </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Brokf0gSU965",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Ploting word cloud\n",
        "start = datetime.now()\n",
        "\n",
        "# Lets first convert the 'result' dictionary to 'list of tuples'\n",
        "tup = dict(result.items())\n",
        "#Initializing WordCloud using frequencies of tags.\n",
        "wordcloud = WordCloud(    background_color='black',\n",
        "                          width=1600,\n",
        "                          height=800,\n",
        "                    ).generate_from_frequencies(tup)\n",
        "\n",
        "fig = plt.figure(figsize=(30,20))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "fig.savefig(\"tag.png\")\n",
        "plt.show()\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_n7RaKj2U96-"
      },
      "source": [
        "<b>Observations:</b><br />\n",
        "A look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hpc8IWjqU96_"
      },
      "source": [
        "<h3> 3.2.6 The top 20 tags </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ov3WmIEHU97A",
        "colab": {}
      },
      "source": [
        "i=np.arange(20)\n",
        "tag_df_sorted.head(20).plot(kind='bar')\n",
        "plt.title('Frequency of top 20 tags')\n",
        "plt.xticks(i, tag_df_sorted['Tags'])\n",
        "plt.xlabel('Tags')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rlmizz7LU97D"
      },
      "source": [
        "<b>Observations:</b><br />\n",
        "1. Majority of the most frequent tags are programming language.\n",
        "2. C# is the top most frequent programming language.\n",
        "3. Android, IOS, Linux and windows are among the top most frequent operating systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-Z7F0_mU97F"
      },
      "source": [
        "<h1> 3.3 Cleaning and preprocessing of Questions </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWzF-nN6U97G"
      },
      "source": [
        "<h3> 3.3.1 Preprocessing </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MeR1aoQaU97H"
      },
      "source": [
        "<ol> \n",
        "    <li> Sample 1M data points </li>\n",
        "    <li> Separate out code-snippets from Body </li>\n",
        "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
        "    <li> Remove stop words (Except 'C') </li>\n",
        "    <li> Remove HTML Tags </li>\n",
        "    <li> Convert all the characters into small letters </li>\n",
        "    <li> Use SnowballStemmer to stem the words </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qr2xhpsAU97I",
        "colab": {}
      },
      "source": [
        "def striphtml(data):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(data))\n",
        "    return cleantext\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCDUa4KxU97L",
        "outputId": "8270b10f-cf17-4025-9440-5f2b2b62579c",
        "colab": {}
      },
      "source": [
        "#******************************************Some functions for databases*****************************************\n",
        "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
        "def create_connection(db_file):\n",
        "    \"\"\" create a database connection to the SQLite database\n",
        "        specified by db_file\n",
        "    :param db_file: database file\n",
        "    :return: Connection object or None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        return conn\n",
        "    except Error as e:\n",
        "        print(e)\n",
        " \n",
        "    return None\n",
        "\n",
        "def create_table(conn, create_table_sql):\n",
        "    \"\"\" create a table from the create_table_sql statement\n",
        "    :param conn: Connection object\n",
        "    :param create_table_sql: a CREATE TABLE statement\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        c = conn.cursor()\n",
        "        c.execute(create_table_sql)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "        \n",
        "def checkTableExists(dbcon):\n",
        "    cursr = dbcon.cursor()\n",
        "    str = \"select name from sqlite_master where type='table'\"\n",
        "    table_names = cursr.execute(str)\n",
        "    print(\"Tables in the databse:\")\n",
        "    tables =table_names.fetchall() \n",
        "    print(tables[0][0])\n",
        "    return(len(tables))\n",
        "\n",
        "def create_database_table(database, query):\n",
        "    conn = create_connection(database)\n",
        "    if conn is not None:\n",
        "        create_table(conn, query)\n",
        "        checkTableExists(conn)\n",
        "    else:\n",
        "        print(\"Error! cannot create the database connection.\")\n",
        "    conn.close()\n",
        "\n",
        "    \n",
        "    \n",
        "#***********************************************Create a databse with the empty table*****************************\n",
        "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
        "create_database_table(\"Processed.db\", sql_create_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qz092AdFU97P",
        "outputId": "b00a3eec-60d8-4c08-83c8-54debd0a4123",
        "colab": {}
      },
      "source": [
        "# http://www.sqlitetutorial.net/sqlite-delete/\n",
        "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
        "start = datetime.now()\n",
        "read_db = 'train_no_dup.db'     # old database which has all the duplicates rows\n",
        "write_db = 'Processed.db'        # new database which i make in this it has one table questions_preprocessed\n",
        "if os.path.isfile(read_db):\n",
        "    conn_r = create_connection(read_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 100000;\")\n",
        "\n",
        "        \n",
        "        \n",
        "  #****************************We get the 100000 datapoints from the  train_no_dup.db database\n",
        "        \n",
        "if os.path.isfile(write_db):\n",
        "    conn_w = create_connection(write_db)\n",
        "    if conn_w is not None:\n",
        "        tables = checkTableExists(conn_w)\n",
        "        writer =conn_w.cursor()\n",
        "        if tables != 0:\n",
        "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")  # rows are empty by the way\n",
        "            print(\"Cleared All the rows\")\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "\n",
        "\n",
        "\n",
        "#****************************Previously we created this table, now we checking if its empty or not, if not emtpy delete al the rows*************************\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n",
            "Cleared All the rows\n",
            "Time taken to run this cell : 0:02:13.422156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWwGMA_MMJD",
        "colab_type": "code",
        "colab": {},
        "outputId": "636eea80-9f04-469e-82e8-dfd454111008"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FELLR5FBU97U"
      },
      "source": [
        "__ we create a new data base to store the sampled and preprocessed questions __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "clKVIuAcU97W",
        "outputId": "eab75c6e-f76e-4cf2-eb4d-c589a9908be5",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
        "\n",
        "start = datetime.now()\n",
        "preprocessed_data_list=[]\n",
        "reader.fetchone()\n",
        "questions_with_code=0\n",
        "len_pre=0\n",
        "len_post=0\n",
        "questions_proccesed = 0\n",
        "for row in reader: # reading one row\n",
        "\n",
        "    is_code = 0\n",
        "\n",
        "    title, question, tags = row[0], row[1], row[2]\n",
        "\n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "    len_pre+=x\n",
        "\n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "\n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=striphtml(question.encode('utf-8'))\n",
        "\n",
        "    title=title.encode('utf-8')\n",
        "\n",
        "    question=str(title)+\" \"+str(question)\n",
        "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "\n",
        "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "\n",
        "    len_post+=len(question)\n",
        "    tup = (question,code,tags,x,len(question),is_code)\n",
        "    questions_proccesed += 1\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #*************************   We are inseting the updated preprocessed data to the new table   'QuestionsProcessed'   ********************\n",
        " \n",
        "    \n",
        "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,   words_pre,   words_post,     is_code) values (?,?,?,?,?,?)\",tup)\n",
        "    if (questions_proccesed%100000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "\n",
        "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
        "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
        "\n",
        "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
        "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
        "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
        "\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. length of questions(Title+Body) before processing: 1175\n",
            "Avg. length of questions(Title+Body) after processing: 326\n",
            "Percent of questions containing code: 57\n",
            "Time taken to run this cell : 0:05:20.314822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fMihWt4uU97b",
        "colab": {}
      },
      "source": [
        "# dont forget to close the connections, or else you will end up with locks\n",
        "conn_r.commit()\n",
        "conn_w.commit()\n",
        "conn_r.close()\n",
        "conn_w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8VzP3hsU97e",
        "outputId": "5efb2876-7d07-438f-aeac-edeecc694f5b",
        "colab": {}
      },
      "source": [
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
        "        print(\"Questions after preprocessed\")\n",
        "        print('='*100)\n",
        "        reader.fetchone()\n",
        "        for row in reader:\n",
        "            print(row)\n",
        "            print('-'*100)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions after preprocessed\n",
            "====================================================================================================\n",
            "('c program dump entir hklm registri tree consol tri write simpl consol app dump content hklm consol output look someth like much luck research help would great appreci',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('android gridview column make ui like net gridview column product name textview product quantiti spinner price textview delet button button delet row question best way control android sdk ui new android think gridview good alreadi follow http www mkyong com android android gridview exampl tri use column spinner show text show littl spinner gridview',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('import databas magento want import tecdoc databas magento without success tecdoc msql format export csv xml problem import product keep schema databas thank',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('exampl libpcap libnet want captur ip packag one server forward packag anoth server libnet exampl thank advanc',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('getscript stylesheet jqueri titl say equival jqueri load stylesheet',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('apach truncat static content tri set moinmoin offic wiki window server run apach origin thought everyth work fine except reason imag display proper turn static serv content get truncat charact tri figur error log show anyth access log say file deliv either ok unchang dynam content seem display ok untrunc django instal server also work normal might caus odd behaviour curious bit math think point encod issu text file number charact miss equal number newlin charact truncat version file chang encod file seem help put content onto one line come fine seem work around issu chang file would bit crufti guess imag addendum see client use wireshark follow tcp stream function main thing notic dynam content bgcwiki number newlin follow static content alway even newlin particular png whitespac end content next get request apach configur file moinmoin pretti standard',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('googl map plot multipl marker array tri plot marker array use code pop current locat write array seem show besid copi part consol log convert array json show show question plot latitud longitud marker map current method seem go queri limit per second thank advanc',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('perl event loop multipl block watcher tri figur event loop perl current program someth like wait event block tri figur use ev anyev ae someth els add anoth event watcher exampl want abl call tri someth everi second current stuck put event loop also would like add form interact program possibl socket anoth watcher thank',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('mvvmlight viewmodelloc regist dataservic question might look naiv understand code viewmodelloc cs file see use dataservic get data wcf servic exampl assign mainviewmodel regist one viewmodel like let say anoth dataservic dataservic exampl one use page viewmodel also someon help even give link read code clue mean',)\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSJXxeS0U97i",
        "colab": {}
      },
      "source": [
        "#****************************From the Processed.db database select the table 'QuestionsProcessed'*************************\n",
        "#Taking 1 Million entries to a dataframe.\n",
        "write_db = 'Processed.db'\n",
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ANsenX1EU97l",
        "outputId": "6170e6c6-86e0-48e9-d1c1-190c0cfc15be",
        "colab": {}
      },
      "source": [
        "preprocessed_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user unabl access site connect vpn one user un...</td>\n",
              "      <td>networking vpn routing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c program dump entir hklm registri tree consol...</td>\n",
              "      <td>c# registry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>android gridview column make ui like net gridv...</td>\n",
              "      <td>android gridview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>import databas magento want import tecdoc data...</td>\n",
              "      <td>database magento import</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>exampl libpcap libnet want captur ip packag on...</td>\n",
              "      <td>linux libpcap libnet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                     tags\n",
              "0  user unabl access site connect vpn one user un...   networking vpn routing\n",
              "1  c program dump entir hklm registri tree consol...              c# registry\n",
              "2  android gridview column make ui like net gridv...         android gridview\n",
              "3  import databas magento want import tecdoc data...  database magento import\n",
              "4  exampl libpcap libnet want captur ip packag on...     linux libpcap libnet"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I6vsCoLOU97r",
        "outputId": "494f221b-f6ed-4aeb-a557-aeb6d209cf38",
        "colab": {}
      },
      "source": [
        "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
        "print(\"number of dimensions :\", preprocessed_data.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of data points in sample : 99999\n",
            "number of dimensions : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qB0bL2drU97w"
      },
      "source": [
        "<h1>4. Machine Learning Models </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZ3-VPbqU97w"
      },
      "source": [
        "<h2> 4.1 Converting tags for multilabel problems </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K88oaTD9U97y"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ttr2m-qiU97z",
        "colab": {}
      },
      "source": [
        "# binary='true' will give a binary vectorizer\n",
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5M0i4naMMLE",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d2b887c-e5eb-4dff-c559-b22e8d5754fc"
      },
      "source": [
        "multilabel_y.shape# we have the total 18585 labels or tags."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99999, 18511)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_VCwyfHxU972"
      },
      "source": [
        "__ We will sample the number of tags instead considering all of them (due to limitation of computing power) __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FtgktWvU973",
        "colab": {}
      },
      "source": [
        "def tags_to_choose(n):\n",
        "    t = multilabel_y.sum(axis=0).tolist()[0]# Frequency of the particular tag             count the columns in the binary vectorizer or bag of words\n",
        "    #print(len(t))\n",
        "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)# sort based on the decending order of tags values (value is number of times it appear)\n",
        "    #print(sorted_tags_i[:n])\n",
        "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]# questions with  the tags(that get in second step) or frequent tags\n",
        "    #print('***********************************************************************')\n",
        "    #print(multilabel_yn)\n",
        "    return multilabel_yn\n",
        "\n",
        "def questions_explained_fn(n):\n",
        "    multilabel_yn = tags_to_choose(n)# tags output that i discussed\n",
        "    x= multilabel_yn.sum(axis=1)#  how many tags a single quesition has !\n",
        "    #print(x)\n",
        "    return ((np.count_nonzero(x==0)))# that questions we not able to explain with the labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxbL2OyqU978",
        "colab": {}
      },
      "source": [
        "questions_explained = []\n",
        "total_tags=multilabel_y.shape[1]\n",
        "total_qs=preprocessed_data.shape[0]\n",
        "for i in range(500, total_tags, 100):\n",
        "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbX6_QWtU98B",
        "outputId": "7add8021-d595-4e9e-c506-611520b21a9c",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(questions_explained)\n",
        "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.xlabel(\"Number of tags\")\n",
        "plt.ylabel(\"Number Questions coverd partially\")\n",
        "\n",
        "plt.grid()\n",
        "plt.show()\n",
        "# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\n",
        "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "with  5500 tags we are covering  99.138 % of questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgJKNRG0U98F",
        "outputId": "b620a1a2-a9c8-43be-b325-a8ff5ec6c0e0",
        "colab": {}
      },
      "source": [
        "multilabel_yx = tags_to_choose(5500)\n",
        "print(\"number of questions that are not covered :\", questions_explained_fn(5500),\"out of \", total_qs)\n",
        "print(multilabel_yx.shape)\n",
        "preprocessed_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions that are not covered : 862 out of  99999\n",
            "(99999, 5500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99999, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BTuhd5XMU98L",
        "outputId": "c60da5fb-99c9-4c8b-f417-633ef97eb598",
        "colab": {}
      },
      "source": [
        "print(\"Number of tags in sample :\", multilabel_y.shape[1])\n",
        "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/multilabel_y.shape[1])*100,\"%)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tags in sample : 18511\n",
            "number of tags taken : 5500 ( 29.71206309761763 %)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxH1ggjxU98R"
      },
      "source": [
        "__ We consider top 15% tags which covers  99% of the questions __"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "stMfn7tMU98U"
      },
      "source": [
        "<h2>4.2 Split the data into test and train (80:20) </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqeDZrwWU98U",
        "colab": {},
        "outputId": "e92081e5-8756-46ec-9af3-4ea5c42253b3"
      },
      "source": [
        "# If we given with the time, we will do teh time split. because tags are changing with the time,, may be first asp.1 versoin we had, now today new version\n",
        "# launched asp.2   . so time based splitting will work here,\n",
        "\n",
        "\n",
        "\n",
        "total_size=preprocessed_data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "x_train=preprocessed_data.head(train_size)\n",
        "x_test=preprocessed_data.tail(total_size - train_size)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "y_train = multilabel_yx[0:train_size,:]\n",
        "y_test = multilabel_yx[train_size:total_size,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79999, 2)\n",
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgNXo4eJU98X",
        "outputId": "21bef306-6c57-47af-e70a-e501c4225d4d",
        "colab": {}
      },
      "source": [
        "print(\"Number of data points in train data :\", y_train.shape)\n",
        "print(\"Number of data points in test data :\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in train data : (79999, 5500)\n",
            "Number of data points in test data : (20000, 5500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "664323eyU98a"
      },
      "source": [
        "<h2>4.3 Featurizing data </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnV3O0WFU98b",
        "outputId": "ef87c28c-4224-451e-c762-0baa2086bd70",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=50000, smooth_idf=True, norm=\"l2\", \\\n",
        "                              sublinear_tf=False, ngram_range=(1,3))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:01:03.359503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CB01RkDzU98f",
        "outputId": "cbeda5ce-bbc3-4d6a-8324-53962dd50d56",
        "colab": {}
      },
      "source": [
        "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of train data X: (79999, 50000) Y : (79999, 5500)\n",
            "Dimensions of test data X: (20000, 50000) Y: (20000, 5500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-JQh1bHU98j",
        "outputId": "63863f36-79ad-4726-de7c-c5d728f8902e",
        "colab": {}
      },
      "source": [
        "# https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
        "#https://stats.stackexchange.com/questions/117796/scikit-multi-label-classification\n",
        "# classifier = LabelPowerset(GaussianNB())\n",
        "\"\"\"\n",
        "from skmultilearn.adapt import MLkNN\n",
        "classifier = MLkNN(k=21)\n",
        "\n",
        "# train\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(x_test_multilabel)\n",
        "print(accuracy_score(y_test,predictions))\n",
        "print(metrics.f1_score(y_test, predictions, average = 'macro'))\n",
        "print(metrics.f1_score(y_test, predictions, average = 'micro'))\n",
        "print(metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\"\"\"\n",
        "# we are getting memory error because the multilearn package \n",
        "# is trying to convert the data into dense matrix\n",
        "# ---------------------------------------------------------------------------\n",
        "#MemoryError                               Traceback (most recent call last)\n",
        "#<ipython-input-170-f0e7c7f3e0be> in <module>()\n",
        "#----> classifier.fit(x_train_multilabel, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom skmultilearn.adapt import MLkNN\\nclassifier = MLkNN(k=21)\\n\\n# train\\nclassifier.fit(x_train_multilabel, y_train)\\n\\n# predict\\npredictions = classifier.predict(x_test_multilabel)\\nprint(accuracy_score(y_test,predictions))\\nprint(metrics.f1_score(y_test, predictions, average = 'macro'))\\nprint(metrics.f1_score(y_test, predictions, average = 'micro'))\\nprint(metrics.hamming_loss(y_test,predictions))\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LvjtTBZ6U98y"
      },
      "source": [
        "<h2> 4.5 Modeling with less data points (0.1M data points) and more weight to title and 500 tags only. </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auOoDQjLMMNN",
        "colab_type": "code",
        "colab": {},
        "outputId": "60d582b8-bb2b-49e3-c72e-83f80aa65855"
      },
      "source": [
        "# Now we'll repeat all the code from the previous sections \n",
        "# procedure\n",
        "#1. Take less datapoints\n",
        "#2. remove the questions and give the high weitage to the title, by just repeating it 3 times.  Also with this we can reduce the dimensions.\n",
        "#3.If we see logically think, users have to write the title so much attractive or Title have to cover the overall view of our error, so it can be useful.\n",
        "\n",
        "\n",
        "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
        "create_database_table(\"Titlemoreweightw.db\", sql_create_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrtiul9pMMNd",
        "colab_type": "code",
        "colab": {},
        "outputId": "607a7af4-facf-4067-a620-6c23c8423440"
      },
      "source": [
        "# http://www.sqlitetutorial.net/sqlite-delete/\n",
        "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
        "\n",
        "read_db = 'train_no_dup.db'\n",
        "write_db = 'Titlemoreweightw.db'\n",
        "train_datasize = 400000\n",
        "if os.path.isfile(read_db):\n",
        "    conn_r = create_connection(read_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        # for selecting first 0.5M rows\n",
        "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train limit 100000;\")\n",
        "        # for selecting random points\n",
        "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
        "\n",
        "if os.path.isfile(write_db):\n",
        "    conn_w = create_connection(write_db)\n",
        "    if conn_w is not None:\n",
        "        tables = checkTableExists(conn_w)\n",
        "        writer =conn_w.cursor()\n",
        "        if tables != 0:\n",
        "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
        "            print(\"Cleared All the rows\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n",
            "Cleared All the rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jvi2298wU986"
      },
      "source": [
        "<h3> 4.5.1 Preprocessing of questions </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JNhcD1LYU987"
      },
      "source": [
        "<ol> \n",
        "    <li> Separate Code from Body </li>\n",
        "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
        "    <li> <b> Give more weightage to title : Add title three times to the question </b> </li>\n",
        "   \n",
        "    <li> Remove stop words (Except 'C') </li>\n",
        "    <li> Remove HTML Tags </li>\n",
        "    <li> Convert all the characters into small letters </li>\n",
        "    <li> Use SnowballStemmer to stem the words </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ifSmL0M-U98-",
        "outputId": "78019f2f-3a06-4b6d-a761-c8cae342ac42",
        "colab": {}
      },
      "source": [
        "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
        "start = datetime.now()\n",
        "preprocessed_data_list=[]\n",
        "reader.fetchone()\n",
        "questions_with_code=0\n",
        "len_pre=0\n",
        "len_post=0\n",
        "questions_proccesed = 0\n",
        "for row in reader:\n",
        "    \n",
        "    is_code = 0\n",
        "    \n",
        "    title, question, tags = row[0], row[1], str(row[2])\n",
        "    \n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "    len_pre+=x\n",
        "    \n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "    \n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=striphtml(question.encode('utf-8'))\n",
        "    \n",
        "    title=title.encode('utf-8')\n",
        "    \n",
        "    # adding title three time to the data to increase its weight\n",
        "    # add tags string to the training data\n",
        "    \n",
        "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "    \n",
        "#     if questions_proccesed<=train_datasize:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
        "#     else:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "\n",
        "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    \n",
        "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    \n",
        "    len_post+=len(question)\n",
        "    tup = (question,code,tags,x,len(question),is_code)\n",
        "    questions_proccesed += 1\n",
        "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
        "    if (questions_proccesed%100000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "\n",
        "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
        "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
        "\n",
        "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
        "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
        "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
        "\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. length of questions(Title+Body) before processing: 1232\n",
            "Avg. length of questions(Title+Body) after processing: 441\n",
            "Percent of questions containing code: 57\n",
            "Time taken to run this cell : 0:07:51.700574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxecdXlJMMN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# never forget to close the conections or else we will end up with database locks\n",
        "conn_r.commit()\n",
        "conn_w.commit()\n",
        "conn_r.close()\n",
        "conn_w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gpN1ZM2bU99F"
      },
      "source": [
        "__ Sample quesitons after preprocessing of data __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ytEecnCtU99H",
        "outputId": "f3c4992d-41e4-4e4a-9ae9-c40bb79dc43a",
        "colab": {}
      },
      "source": [
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
        "        print(\"Questions after preprocessed\")\n",
        "        print('='*100)\n",
        "        reader.fetchone()\n",
        "        for row in reader:\n",
        "            print(row)\n",
        "            print('-'*100)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions after preprocessed\n",
            "====================================================================================================\n",
            "('dynam datagrid bind silverlight dynam datagrid bind silverlight dynam datagrid bind silverlight bind datagrid dynam code wrote code debug code block seem bind correct grid come column form come grid column although necessari bind nthank repli advance..',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('java.lang.noclassdeffounderror javax servlet jsp tagext taglibraryvalid java.lang.noclassdeffounderror javax servlet jsp tagext taglibraryvalid java.lang.noclassdeffounderror javax servlet jsp tagext taglibraryvalid follow guid link instal jstl got follow error tri launch jsp page java.lang.noclassdeffounderror javax servlet jsp tagext taglibraryvalid taglib declar instal jstl 1.1 tomcat webapp tri project work also tri version 1.2 jstl still messag caus solv',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index use follow code display caus solv',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('better way updat feed fb php sdk better way updat feed fb php sdk better way updat feed fb php sdk novic facebook api read mani tutori still confused.i find post feed api method like correct second way use curl someth like way better',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('btnadd click event open two window record ad btnadd click event open two window record ad btnadd click event open two window record ad open window search.aspx use code hav add button search.aspx nwhen insert record btnadd click event open anoth window nafter insert record close window',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('sql inject issu prevent correct form submiss php sql inject issu prevent correct form submiss php sql inject issu prevent correct form submiss php check everyth think make sure input field safe type sql inject good news safe bad news one tag mess form submiss place even touch life figur exact html use templat file forgiv okay entir php script get execut see data post none forum field post problem use someth titl field none data get post current use print post see submit noth work flawless statement though also mention script work flawless local machin use host come across problem state list input test mess',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('countabl subaddit lebesgu measur countabl subaddit lebesgu measur countabl subaddit lebesgu measur let lbrace rbrace sequenc set sigma -algebra mathcal want show left bigcup right leq sum left right countabl addit measur defin set sigma algebra mathcal think use monoton properti somewher proof start appreci littl help nthank ad han answer make follow addit construct given han answer clear bigcup bigcup cap emptyset neq left bigcup right left bigcup right sum left right also construct subset monoton left right leq left right final would sum leq sum result follow',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('hql equival sql queri hql equival sql queri hql equival sql queri hql queri replac name class properti name error occur hql error',)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "('undefin symbol architectur i386 objc class skpsmtpmessag referenc error undefin symbol architectur i386 objc class skpsmtpmessag referenc error undefin symbol architectur i386 objc class skpsmtpmessag referenc error import framework send email applic background import framework i.e skpsmtpmessag somebodi suggest get error collect2 ld return exit status import framework correct sorc taken framework follow mfmailcomposeviewcontrol question lock field updat answer drag drop folder project click copi nthat',)\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IspyyegoU99N"
      },
      "source": [
        "__ Saving Preprocessed data to a Database __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F_x-ETQJU99P",
        "colab": {}
      },
      "source": [
        "#Taking 0.5 Million entries to a dataframe.\n",
        "write_db = 'Titlemoreweightw.db'\n",
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bc7hwHjBU99U",
        "outputId": "7a90802a-0630-44da-853d-987e6ece8c4b",
        "colab": {}
      },
      "source": [
        "preprocessed_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99999, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xk9V0azqU99X",
        "outputId": "c767de50-461e-4454-c634-5487aa543b82",
        "colab": {}
      },
      "source": [
        "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
        "print(\"number of dimensions :\", preprocessed_data.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of data points in sample : 99999\n",
            "number of dimensions : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oUpccCSkU99Z"
      },
      "source": [
        "__ Converting string Tags to multilable output variables __ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SWg_g1lNU99a",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(binary='true')\n",
        "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pbtD0Hx8U99c"
      },
      "source": [
        "__ Selecting 500 Tags __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_nMDxAIU99d",
        "colab": {}
      },
      "source": [
        "questions_explained = []\n",
        "total_tags=multilabel_y.shape[1]\n",
        "total_qs=preprocessed_data.shape[0]\n",
        "for i in range(500, total_tags, 100):\n",
        "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fggMk2IJU99f",
        "outputId": "7c443492-b0c4-492d-afc2-16c59b8b954e",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(questions_explained)\n",
        "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.xlabel(\"Number of tags\")\n",
        "plt.ylabel(\"Number Questions coverd partially\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
        "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
        "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "with  5500 tags we are covering  98.986 % of questions\n",
            "with  500 tags we are covering  93.743 % of questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VuJzfmNrU99i",
        "outputId": "2c34ec8f-ee38-451d-f477-89f880a98b7d",
        "colab": {}
      },
      "source": [
        "# we will be taking 500 tags\n",
        "multilabel_yx = tags_to_choose(500)\n",
        "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions that are not covered : 6257 out of  99999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoUGVC7zMMPe",
        "colab_type": "code",
        "colab": {},
        "outputId": "303e6438-4d76-4915-d328-e0ecaa4c4982"
      },
      "source": [
        "preprocessed_data.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WsduwXTeU99k",
        "colab": {},
        "outputId": "5f3b54d7-6d77-4a96-dc65-0d3cf9b34a35"
      },
      "source": [
        "# If we given with the time, we will do teh time split. because tags are changing with the time,, may be first asp.1 versoin we had, now today new version\n",
        "# launched asp.2   . so time based splitting will work here,\n",
        "\n",
        "\n",
        "total_size=preprocessed_data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "\n",
        "x_train=preprocessed_data.head(train_size)\n",
        "x_test=preprocessed_data.tail(total_size - train_size)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "y_train = multilabel_yx[0:train_size,:]\n",
        "y_test = multilabel_yx[train_size:total_size,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79999, 2)\n",
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iZZDSH_VU99m",
        "outputId": "15b74fe3-29ac-40d2-c393-82260f78ce19",
        "colab": {}
      },
      "source": [
        "print(\"Number of data points in train data :\", y_train.shape)\n",
        "print(\"Number of data points in test data :\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in train data : (79999, 500)\n",
            "Number of data points in test data : (20000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gDJ2PvnzU99o"
      },
      "source": [
        "<h3> 4.5.2 Featurizing data with TfIdf vectorizer </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "530e8tW9U99o",
        "outputId": "a73463fb-694d-44bf-dc16-97c9b161f0a0",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=10000, smooth_idf=True, norm=\"l2\", sublinear_tf=False, ngram_range=(1,3))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:01:16.136466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r9iDfzXIU99t",
        "outputId": "23c4dfde-72c4-40c3-9339-29e56fc18b59",
        "colab": {}
      },
      "source": [
        "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of train data X: (79999, 10000) Y : (79999, 500)\n",
            "Dimensions of test data X: (20000, 10000) Y: (20000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kmmnoy4XU99v"
      },
      "source": [
        "<h3> 4.5.3 Applying Logistic Regression with OneVsRest Classifier </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnHoxl5DU99w",
        "outputId": "fea313da-ed92-469d-d34b-149d3fc5e01e",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.1937\n",
            "Hamming loss  0.0035708\n",
            "Micro-average quality numbers\n",
            "Precision: 0.7346, Recall: 0.3800, F1-measure: 0.5009\n",
            "Macro-average quality numbers\n",
            "Precision: 0.5558, Recall: 0.2813, F1-measure: 0.3510\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.47      0.59      1805\n",
            "           1       0.86      0.53      0.65      1186\n",
            "           2       0.87      0.55      0.68       484\n",
            "           3       0.82      0.46      0.59      1323\n",
            "           4       0.87      0.60      0.71       739\n",
            "           5       0.87      0.48      0.62      1023\n",
            "           6       0.77      0.39      0.52      1421\n",
            "           7       0.95      0.62      0.75      1450\n",
            "           8       0.98      0.82      0.89      1368\n",
            "           9       0.68      0.45      0.54       914\n",
            "          10       0.80      0.41      0.55       186\n",
            "          11       0.77      0.49      0.60       553\n",
            "          12       0.78      0.40      0.53       644\n",
            "          13       0.52      0.19      0.28       424\n",
            "          14       0.70      0.39      0.50        36\n",
            "          15       0.59      0.37      0.45       352\n",
            "          16       0.64      0.23      0.34       437\n",
            "          17       0.76      0.46      0.57       435\n",
            "          18       0.68      0.56      0.61       153\n",
            "          19       0.98      0.60      0.75       727\n",
            "          20       0.63      0.19      0.30       488\n",
            "          21       0.85      0.62      0.72       272\n",
            "          22       0.92      0.58      0.71       530\n",
            "          23       0.95      0.54      0.69       618\n",
            "          24       0.96      0.55      0.70       614\n",
            "          25       0.68      0.29      0.40       231\n",
            "          26       0.53      0.33      0.41       588\n",
            "          27       0.58      0.40      0.47      1224\n",
            "          28       0.71      0.45      0.55       165\n",
            "          29       0.62      0.54      0.58       231\n",
            "          30       0.72      0.28      0.40       190\n",
            "          31       0.82      0.59      0.69       296\n",
            "          32       0.69      0.34      0.46       274\n",
            "          33       0.56      0.38      0.45       292\n",
            "          34       0.73      0.27      0.40       190\n",
            "          35       0.86      0.44      0.59        99\n",
            "          36       0.88      0.59      0.71       357\n",
            "          37       0.69      0.38      0.49       870\n",
            "          38       0.81      0.47      0.60       135\n",
            "          39       1.00      0.35      0.52        17\n",
            "          40       0.53      0.08      0.14        99\n",
            "          41       0.67      0.29      0.40       176\n",
            "          42       0.29      0.05      0.09       236\n",
            "          43       0.88      0.32      0.47        22\n",
            "          44       0.53      0.19      0.28       106\n",
            "          45       0.56      0.13      0.22       178\n",
            "          46       0.43      0.24      0.30       241\n",
            "          47       0.64      0.17      0.27       217\n",
            "          48       0.64      0.49      0.55       223\n",
            "          49       0.67      0.07      0.13        54\n",
            "          50       0.62      0.35      0.44        92\n",
            "          51       0.86      0.59      0.70       203\n",
            "          52       0.71      0.47      0.57       116\n",
            "          53       0.81      0.49      0.61        72\n",
            "          54       0.38      0.20      0.26        15\n",
            "          55       0.25      0.02      0.03        60\n",
            "          56       0.90      0.79      0.84       216\n",
            "          57       0.35      0.08      0.13        74\n",
            "          58       0.35      0.13      0.19       139\n",
            "          59       0.71      0.45      0.55        91\n",
            "          60       0.48      0.10      0.17       156\n",
            "          61       0.42      0.33      0.37        76\n",
            "          62       0.52      0.18      0.27        89\n",
            "          63       0.48      0.17      0.25       173\n",
            "          64       0.53      0.28      0.36       227\n",
            "          65       0.45      0.11      0.18       383\n",
            "          66       0.65      0.22      0.32       148\n",
            "          67       0.56      0.40      0.46       189\n",
            "          68       0.75      0.35      0.48       169\n",
            "          69       0.14      0.06      0.08        50\n",
            "          70       0.68      0.26      0.38       145\n",
            "          71       0.42      0.26      0.32        31\n",
            "          72       0.93      0.72      0.81       141\n",
            "          73       0.88      0.43      0.58       246\n",
            "          74       0.54      0.30      0.39       210\n",
            "          75       0.70      0.10      0.18       159\n",
            "          76       0.49      0.21      0.30       108\n",
            "          77       0.94      0.78      0.86        65\n",
            "          78       0.97      0.70      0.81       145\n",
            "          79       0.91      0.71      0.79        41\n",
            "          80       0.73      0.57      0.64       129\n",
            "          81       0.89      0.53      0.66        76\n",
            "          82       0.63      0.45      0.53       124\n",
            "          83       0.41      0.13      0.20        69\n",
            "          84       0.44      0.16      0.24        91\n",
            "          85       0.49      0.42      0.46        66\n",
            "          86       0.21      0.08      0.12       100\n",
            "          87       0.43      0.26      0.33        38\n",
            "          88       0.73      0.45      0.56        98\n",
            "          89       0.52      0.39      0.45        38\n",
            "          90       0.97      0.68      0.80       154\n",
            "          91       0.88      0.65      0.75       152\n",
            "          92       0.00      0.00      0.00        13\n",
            "          93       0.00      0.00      0.00        47\n",
            "          94       0.80      0.27      0.41        44\n",
            "          95       0.78      0.29      0.43       200\n",
            "          96       0.40      0.24      0.30        25\n",
            "          97       0.61      0.28      0.39        39\n",
            "          98       0.58      0.43      0.49        51\n",
            "          99       0.35      0.26      0.30        43\n",
            "         100       0.33      0.11      0.16       211\n",
            "         101       0.57      0.22      0.32        18\n",
            "         102       0.67      0.50      0.57        32\n",
            "         103       0.77      0.42      0.54        24\n",
            "         104       0.80      0.29      0.42        14\n",
            "         105       0.70      0.48      0.57        96\n",
            "         106       1.00      0.41      0.58        32\n",
            "         107       0.60      0.38      0.46        80\n",
            "         108       0.74      0.19      0.31       160\n",
            "         109       0.39      0.07      0.12       123\n",
            "         110       0.37      0.05      0.09       202\n",
            "         111       0.56      0.46      0.51        39\n",
            "         112       0.35      0.07      0.11       123\n",
            "         113       0.71      0.53      0.60        55\n",
            "         114       0.45      0.13      0.20        98\n",
            "         115       0.35      0.16      0.22        50\n",
            "         116       0.84      0.54      0.65       275\n",
            "         117       0.40      0.04      0.07       101\n",
            "         118       0.67      0.12      0.20        50\n",
            "         119       0.57      0.20      0.29        41\n",
            "         120       0.62      0.27      0.37        98\n",
            "         121       0.44      0.13      0.21        30\n",
            "         122       0.83      0.33      0.47        73\n",
            "         123       0.91      0.79      0.85       121\n",
            "         124       0.55      0.38      0.45        29\n",
            "         125       0.92      0.21      0.34        57\n",
            "         126       0.50      0.15      0.23        48\n",
            "         127       0.90      0.75      0.82        24\n",
            "         128       0.48      0.25      0.33        48\n",
            "         129       0.75      0.19      0.30        48\n",
            "         130       0.89      0.51      0.65        99\n",
            "         131       0.50      0.38      0.43        29\n",
            "         132       0.45      0.08      0.14        60\n",
            "         133       0.71      0.74      0.73        89\n",
            "         134       0.36      0.04      0.08       113\n",
            "         135       0.38      0.13      0.19        70\n",
            "         136       0.38      0.07      0.12        68\n",
            "         137       0.94      0.55      0.70       146\n",
            "         138       0.79      0.33      0.47        66\n",
            "         139       0.38      0.06      0.11        49\n",
            "         140       0.89      0.47      0.62        51\n",
            "         141       0.56      0.33      0.42        27\n",
            "         142       0.20      0.04      0.06        54\n",
            "         143       0.50      0.10      0.16        21\n",
            "         144       0.40      0.14      0.21        43\n",
            "         145       0.95      0.41      0.57        49\n",
            "         146       0.64      0.54      0.58       137\n",
            "         147       0.84      0.47      0.61        91\n",
            "         148       0.48      0.34      0.40        29\n",
            "         149       0.95      0.62      0.75        88\n",
            "         150       0.70      0.10      0.18        67\n",
            "         151       0.70      0.41      0.52        46\n",
            "         152       0.59      0.33      0.42       187\n",
            "         153       0.81      0.42      0.55        60\n",
            "         154       0.83      0.38      0.52        40\n",
            "         155       0.38      0.04      0.08        67\n",
            "         156       0.33      0.11      0.16        46\n",
            "         157       0.64      0.30      0.41        23\n",
            "         158       0.68      0.50      0.57        54\n",
            "         159       0.46      0.37      0.41        87\n",
            "         160       0.70      0.21      0.33        66\n",
            "         161       0.88      0.54      0.67        69\n",
            "         162       0.41      0.15      0.22        78\n",
            "         163       0.98      0.82      0.89        50\n",
            "         164       0.39      0.11      0.18       115\n",
            "         165       0.65      0.18      0.29        71\n",
            "         166       0.12      0.01      0.02        81\n",
            "         167       0.40      0.52      0.45        52\n",
            "         168       0.62      0.36      0.46        22\n",
            "         169       0.00      0.00      0.00       292\n",
            "         170       0.32      0.40      0.35        45\n",
            "         171       0.31      0.03      0.06       146\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.53      0.30      0.38        66\n",
            "         174       0.30      0.14      0.19        21\n",
            "         175       0.50      0.08      0.13        26\n",
            "         176       0.42      0.09      0.15        86\n",
            "         177       0.43      0.17      0.24        18\n",
            "         178       0.12      0.04      0.06        27\n",
            "         179       0.00      0.00      0.00         0\n",
            "         180       1.00      0.71      0.83         7\n",
            "         181       1.00      0.53      0.69        34\n",
            "         182       0.73      0.63      0.68        35\n",
            "         183       0.68      0.51      0.58        51\n",
            "         184       0.89      0.63      0.74        38\n",
            "         185       0.20      0.05      0.08        39\n",
            "         186       0.50      0.08      0.13        13\n",
            "         187       0.60      0.34      0.44        35\n",
            "         188       0.31      0.11      0.17        44\n",
            "         189       0.50      0.11      0.18        46\n",
            "         190       0.69      0.17      0.28        52\n",
            "         191       0.48      0.11      0.18        88\n",
            "         192       0.25      0.02      0.04        41\n",
            "         193       0.96      0.53      0.69        88\n",
            "         194       0.50      0.04      0.07        51\n",
            "         195       0.55      0.20      0.30       127\n",
            "         196       0.00      0.00      0.00        60\n",
            "         197       1.00      0.17      0.29        18\n",
            "         198       0.33      0.03      0.05        36\n",
            "         199       0.19      0.04      0.06        85\n",
            "         200       0.50      0.19      0.27        48\n",
            "         201       0.45      0.29      0.36        17\n",
            "         202       0.40      0.22      0.29        27\n",
            "         203       0.65      0.18      0.29        60\n",
            "         204       0.82      0.50      0.62       105\n",
            "         205       0.64      0.50      0.56        50\n",
            "         206       0.55      0.27      0.36        45\n",
            "         207       0.40      0.32      0.35        19\n",
            "         208       0.57      0.27      0.37        73\n",
            "         209       0.00      0.00      0.00        51\n",
            "         210       0.80      0.20      0.32        20\n",
            "         211       0.00      0.00      0.00        47\n",
            "         212       0.00      0.00      0.00        44\n",
            "         213       0.63      0.35      0.45        34\n",
            "         214       0.72      0.49      0.58       106\n",
            "         215       0.79      0.44      0.57        59\n",
            "         216       0.33      0.10      0.16        87\n",
            "         217       0.80      0.26      0.39        31\n",
            "         218       0.74      0.61      0.67        46\n",
            "         219       0.60      0.11      0.19        27\n",
            "         220       0.27      0.08      0.12        39\n",
            "         221       0.75      0.38      0.51        55\n",
            "         222       0.67      0.12      0.20        34\n",
            "         223       0.67      0.36      0.47        11\n",
            "         224       0.35      0.12      0.18        51\n",
            "         225       0.18      0.07      0.10        46\n",
            "         226       0.50      0.09      0.15        47\n",
            "         227       0.25      0.07      0.11        14\n",
            "         228       0.83      0.24      0.37        21\n",
            "         229       0.62      0.07      0.13        67\n",
            "         230       0.00      0.00      0.00       229\n",
            "         231       0.67      0.11      0.19        54\n",
            "         232       0.77      0.10      0.18        98\n",
            "         233       0.92      0.43      0.59        53\n",
            "         234       0.57      0.22      0.32        36\n",
            "         235       0.68      0.47      0.56        53\n",
            "         236       0.51      0.34      0.41        68\n",
            "         237       0.31      0.13      0.19        38\n",
            "         238       0.46      0.11      0.17       102\n",
            "         239       0.33      0.33      0.33         6\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.50      0.33      0.40         3\n",
            "         242       0.50      0.13      0.21        68\n",
            "         243       0.50      0.43      0.46        91\n",
            "         244       0.92      0.73      0.81        30\n",
            "         245       0.79      0.22      0.34        50\n",
            "         246       1.00      0.25      0.40         4\n",
            "         247       0.65      0.27      0.38        41\n",
            "         248       0.64      0.21      0.32        98\n",
            "         249       0.00      0.00      0.00         0\n",
            "         250       1.00      1.00      1.00         1\n",
            "         251       1.00      0.19      0.32        26\n",
            "         252       0.66      0.29      0.40        66\n",
            "         253       0.79      0.66      0.72        67\n",
            "         254       0.00      0.00      0.00        32\n",
            "         255       0.00      0.00      0.00         2\n",
            "         256       0.60      0.09      0.16        32\n",
            "         257       1.00      0.50      0.67         4\n",
            "         258       0.75      0.08      0.14        39\n",
            "         259       0.85      0.45      0.59        73\n",
            "         260       1.00      0.60      0.75        55\n",
            "         261       0.50      0.33      0.40        12\n",
            "         262       0.44      0.27      0.33        41\n",
            "         263       0.71      0.36      0.48        14\n",
            "         264       0.69      0.16      0.26        56\n",
            "         265       0.86      0.23      0.37        77\n",
            "         266       0.00      0.00      0.00        13\n",
            "         267       0.45      0.31      0.37        16\n",
            "         268       0.00      0.00      0.00        34\n",
            "         269       0.00      0.00      0.00        45\n",
            "         270       1.00      0.07      0.13        43\n",
            "         271       0.44      0.29      0.35        56\n",
            "         272       0.60      0.27      0.37        11\n",
            "         273       0.00      0.00      0.00        42\n",
            "         274       0.85      0.63      0.72        35\n",
            "         275       0.44      0.07      0.12        59\n",
            "         276       0.29      0.10      0.15        49\n",
            "         277       0.63      0.66      0.64        44\n",
            "         278       0.56      0.11      0.18        46\n",
            "         279       0.00      0.00      0.00         7\n",
            "         280       0.88      0.66      0.75        58\n",
            "         281       0.67      0.35      0.46        46\n",
            "         282       0.36      0.40      0.38        10\n",
            "         283       0.58      0.33      0.42        21\n",
            "         284       0.25      0.04      0.07        47\n",
            "         285       0.57      0.17      0.27        23\n",
            "         286       0.92      0.69      0.79        48\n",
            "         287       0.58      0.60      0.59        35\n",
            "         288       0.15      0.02      0.04        81\n",
            "         289       0.73      0.47      0.57        47\n",
            "         290       0.73      0.71      0.72        93\n",
            "         291       0.10      0.02      0.03        61\n",
            "         292       0.70      0.61      0.65        23\n",
            "         293       0.83      0.50      0.62        10\n",
            "         294       0.50      0.03      0.06        30\n",
            "         295       0.00      0.00      0.00        24\n",
            "         296       0.00      0.00      0.00        54\n",
            "         297       0.56      0.65      0.60        34\n",
            "         298       0.37      0.33      0.35        69\n",
            "         299       0.87      0.75      0.80        44\n",
            "         300       0.71      0.38      0.50        13\n",
            "         301       0.88      0.54      0.67        68\n",
            "         302       0.00      0.00      0.00        33\n",
            "         303       0.62      0.44      0.52        18\n",
            "         304       0.20      0.08      0.11        13\n",
            "         305       0.75      0.34      0.47        53\n",
            "         306       0.73      0.21      0.33        75\n",
            "         307       0.88      0.53      0.66        55\n",
            "         308       0.95      0.61      0.74        61\n",
            "         309       0.80      0.41      0.54        90\n",
            "         310       0.56      0.09      0.15        58\n",
            "         311       0.89      0.84      0.86        19\n",
            "         312       0.67      0.06      0.11        34\n",
            "         313       0.40      0.31      0.35        13\n",
            "         314       0.20      0.25      0.22         4\n",
            "         315       0.44      0.10      0.16        41\n",
            "         316       0.81      0.41      0.54        54\n",
            "         317       0.86      0.24      0.38        25\n",
            "         318       0.20      0.25      0.22         4\n",
            "         319       0.40      0.07      0.12        29\n",
            "         320       0.62      0.22      0.32        37\n",
            "         321       1.00      0.17      0.29         6\n",
            "         322       0.14      0.05      0.07        22\n",
            "         323       0.25      0.05      0.09        19\n",
            "         324       0.20      0.25      0.22         4\n",
            "         325       0.54      0.39      0.45        18\n",
            "         326       0.75      0.43      0.55        21\n",
            "         327       0.00      0.00      0.00        26\n",
            "         328       0.72      0.47      0.57        49\n",
            "         329       0.61      0.54      0.58        35\n",
            "         330       1.00      0.05      0.10        19\n",
            "         331       0.60      0.20      0.30        15\n",
            "         332       0.00      0.00      0.00        10\n",
            "         333       0.74      0.53      0.62        38\n",
            "         334       0.14      0.11      0.12         9\n",
            "         335       0.60      0.06      0.10        53\n",
            "         336       1.00      0.56      0.72        32\n",
            "         337       0.33      0.04      0.07        24\n",
            "         338       1.00      0.67      0.80         3\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         0\n",
            "         341       0.71      0.45      0.56        11\n",
            "         342       0.68      0.47      0.56        40\n",
            "         343       0.00      0.00      0.00        30\n",
            "         344       0.40      0.08      0.14        24\n",
            "         345       0.50      0.04      0.08        23\n",
            "         346       0.61      0.28      0.38        69\n",
            "         347       0.20      0.06      0.09        18\n",
            "         348       0.17      0.03      0.05        65\n",
            "         349       0.47      0.23      0.31        78\n",
            "         350       1.00      0.08      0.15        12\n",
            "         351       0.50      0.08      0.13        13\n",
            "         352       0.40      0.11      0.17        18\n",
            "         353       1.00      0.63      0.77        46\n",
            "         354       0.82      0.57      0.68        40\n",
            "         355       0.00      0.00      0.00        19\n",
            "         356       0.67      0.08      0.14        26\n",
            "         357       0.53      0.23      0.32        39\n",
            "         358       1.00      0.17      0.29        12\n",
            "         359       0.60      0.19      0.29        16\n",
            "         360       0.70      0.29      0.41        24\n",
            "         361       0.33      0.11      0.16        57\n",
            "         362       0.84      0.80      0.82        20\n",
            "         363       0.83      0.06      0.11        84\n",
            "         364       0.73      0.65      0.69        54\n",
            "         365       0.44      0.12      0.19        33\n",
            "         366       0.67      0.13      0.22        30\n",
            "         367       1.00      0.07      0.12        30\n",
            "         368       0.20      0.05      0.08        19\n",
            "         369       0.00      0.00      0.00        19\n",
            "         370       1.00      0.03      0.06        32\n",
            "         371       0.62      0.42      0.50        12\n",
            "         372       0.50      0.07      0.12        15\n",
            "         373       0.12      0.07      0.09        15\n",
            "         374       0.92      0.65      0.76        17\n",
            "         375       1.00      0.66      0.79        41\n",
            "         376       0.94      0.55      0.70        29\n",
            "         377       0.00      0.00      0.00        28\n",
            "         378       0.50      0.16      0.24        19\n",
            "         379       0.40      0.06      0.11        31\n",
            "         380       0.67      0.14      0.23        29\n",
            "         381       0.29      0.08      0.13        49\n",
            "         382       0.00      0.00      0.00         8\n",
            "         383       0.29      0.08      0.13        24\n",
            "         384       0.50      0.35      0.41        20\n",
            "         385       0.00      0.00      0.00        15\n",
            "         386       0.81      0.57      0.67        37\n",
            "         387       0.00      0.00      0.00        22\n",
            "         388       1.00      0.04      0.07        27\n",
            "         389       0.50      0.38      0.43        29\n",
            "         390       0.00      0.00      0.00        20\n",
            "         391       0.72      0.54      0.62        39\n",
            "         392       0.50      0.10      0.17        10\n",
            "         393       0.38      0.14      0.21        42\n",
            "         394       0.67      0.09      0.15        46\n",
            "         395       0.10      0.10      0.10        10\n",
            "         396       0.75      0.08      0.14        39\n",
            "         397       0.00      0.00      0.00        43\n",
            "         398       0.71      0.30      0.42        50\n",
            "         399       1.00      0.57      0.73         7\n",
            "         400       0.25      0.06      0.10        17\n",
            "         401       1.00      0.17      0.29         6\n",
            "         402       0.00      0.00      0.00        26\n",
            "         403       1.00      0.10      0.18        10\n",
            "         404       0.71      0.36      0.48        14\n",
            "         405       0.00      0.00      0.00        14\n",
            "         406       0.82      0.41      0.55        22\n",
            "         407       0.62      0.17      0.26        60\n",
            "         408       0.39      0.17      0.24        40\n",
            "         409       0.00      0.00      0.00        31\n",
            "         410       0.38      0.33      0.35         9\n",
            "         411       0.42      0.26      0.32        19\n",
            "         412       0.67      0.53      0.59        19\n",
            "         413       0.50      0.20      0.29         5\n",
            "         414       0.33      0.08      0.13        12\n",
            "         415       1.00      0.66      0.79        29\n",
            "         416       0.67      0.06      0.11        33\n",
            "         417       0.33      0.03      0.06        33\n",
            "         418       0.40      0.17      0.24        12\n",
            "         419       0.36      0.10      0.15        42\n",
            "         420       0.50      0.58      0.54        12\n",
            "         421       0.33      0.18      0.24        98\n",
            "         422       0.33      0.12      0.18         8\n",
            "         423       0.00      0.00      0.00         7\n",
            "         424       0.75      0.46      0.57        13\n",
            "         425       0.33      0.08      0.12        13\n",
            "         426       0.33      0.10      0.15        20\n",
            "         427       0.25      0.05      0.09        58\n",
            "         428       0.67      1.00      0.80         2\n",
            "         429       0.38      0.30      0.33        27\n",
            "         430       0.50      0.37      0.42        38\n",
            "         431       0.56      0.23      0.32        40\n",
            "         432       1.00      0.05      0.09        43\n",
            "         433       0.96      0.57      0.72        42\n",
            "         434       0.64      0.29      0.40        24\n",
            "         435       0.33      0.03      0.06        31\n",
            "         436       0.40      0.33      0.36        30\n",
            "         437       0.25      0.06      0.10        16\n",
            "         438       0.67      0.45      0.54        22\n",
            "         439       1.00      1.00      1.00         1\n",
            "         440       0.17      0.11      0.13        19\n",
            "         441       0.67      0.22      0.33         9\n",
            "         442       0.33      0.11      0.17       100\n",
            "         443       0.83      0.36      0.50        28\n",
            "         444       0.75      0.60      0.67        20\n",
            "         445       0.45      0.45      0.45        29\n",
            "         446       0.00      0.00      0.00        21\n",
            "         447       0.80      0.20      0.32        20\n",
            "         448       0.88      0.55      0.68        38\n",
            "         449       0.00      0.00      0.00        22\n",
            "         450       0.61      0.52      0.56        21\n",
            "         451       0.00      0.00      0.00        13\n",
            "         452       0.00      0.00      0.00        24\n",
            "         453       0.55      0.12      0.20        48\n",
            "         454       0.47      0.11      0.17        75\n",
            "         455       0.00      0.00      0.00        18\n",
            "         456       0.00      0.00      0.00         3\n",
            "         457       0.55      0.46      0.50        13\n",
            "         458       0.50      0.15      0.24        13\n",
            "         459       0.27      0.25      0.26        24\n",
            "         460       0.62      0.28      0.38        36\n",
            "         461       0.64      0.50      0.56        18\n",
            "         462       0.50      0.23      0.31        31\n",
            "         463       0.67      0.07      0.13        28\n",
            "         464       0.00      0.00      0.00         7\n",
            "         465       0.89      0.30      0.44        27\n",
            "         466       1.00      0.83      0.91        12\n",
            "         467       0.67      0.14      0.24        14\n",
            "         468       0.00      0.00      0.00         6\n",
            "         469       0.27      0.18      0.21        17\n",
            "         470       0.30      0.17      0.21        18\n",
            "         471       0.67      0.07      0.12        29\n",
            "         472       0.00      0.00      0.00         2\n",
            "         473       0.38      0.09      0.14        34\n",
            "         474       0.00      0.00      0.00         8\n",
            "         475       0.25      0.25      0.25         4\n",
            "         476       0.69      0.50      0.58        22\n",
            "         477       0.50      0.67      0.57         6\n",
            "         478       0.33      0.24      0.28        17\n",
            "         479       0.00      0.00      0.00        23\n",
            "         480       0.86      0.33      0.48        18\n",
            "         481       0.83      0.45      0.59        11\n",
            "         482       1.00      0.29      0.44        35\n",
            "         483       0.59      0.62      0.60        21\n",
            "         484       0.86      0.64      0.73        28\n",
            "         485       0.62      0.36      0.45        14\n",
            "         486       0.90      0.82      0.86        11\n",
            "         487       1.00      0.13      0.24        15\n",
            "         488       0.58      0.18      0.28        38\n",
            "         489       0.08      0.01      0.02        75\n",
            "         490       0.97      0.57      0.72        51\n",
            "         491       1.00      0.68      0.81        19\n",
            "         492       0.50      0.19      0.28        21\n",
            "         493       0.67      0.12      0.21        16\n",
            "         494       1.00      0.83      0.91         6\n",
            "         495       0.40      0.18      0.25        22\n",
            "         496       0.68      0.35      0.46        37\n",
            "         497       0.29      0.20      0.24        20\n",
            "         498       0.70      0.58      0.64        24\n",
            "         499       0.00      0.00      0.00        17\n",
            "\n",
            "   micro avg       0.73      0.38      0.50     47151\n",
            "   macro avg       0.56      0.28      0.35     47151\n",
            "weighted avg       0.68      0.38      0.47     47151\n",
            " samples avg       0.51      0.37      0.40     47151\n",
            "\n",
            "Time taken to run this cell : 0:02:47.896331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6qY1LzZPU991",
        "colab": {}
      },
      "source": [
        "# For saving the weights or results after run applying model\n",
        "#joblib.dump(classifier, 'lr_with_more_title_weight.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDaGCH83MMQ1",
        "colab_type": "text"
      },
      "source": [
        "<h1> 5. Assignments </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg1fDYjJMMRK",
        "colab_type": "text"
      },
      "source": [
        "<ol>\n",
        "    <li> Use bag of words upto 4 grams and compute the micro f1 score with Logistic regression(OvR) </li>\n",
        "    <li> Perform hyperparam tuning on alpha (or lambda) for Logistic regression to improve the performance using GridSearch  </li>\n",
        "    <li> Try OneVsRestClassifier  with Linear-SVM (SGDClassifier with loss-hinge)</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFh242KUMMRM",
        "colab_type": "text"
      },
      "source": [
        "<h3> 4.5.2 Featurizing data with BOW vectorizer </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxvvyOW1MMRP",
        "colab_type": "code",
        "colab": {},
        "outputId": "1a26199e-3e08-43ea-b24b-e71ba8ff4106"
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer =  CountVectorizer(min_df=0.00009, max_features=10000, ngram_range=(1,4))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:02:01.153712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShlorccAMMRX",
        "colab_type": "code",
        "colab": {},
        "outputId": "a36e716b-2cf1-460e-cb37-1dc98edb73ba"
      },
      "source": [
        "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of train data X: (79999, 10000) Y : (79999, 500)\n",
            "Dimensions of test data X: (20000, 10000) Y: (20000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUaDFHgTMMRv",
        "colab_type": "text"
      },
      "source": [
        "<h1>Hyperparameter tuning:</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui8rrFj_MMRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt \n",
        "#from sklearn.grid_search import GridSearchCV\"\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLKI0irIMMR5",
        "colab_type": "code",
        "colab": {},
        "outputId": "9efd9df9-e7c1-4b2e-977c-ee70494539d4"
      },
      "source": [
        "alpha =[10**-5,10**-4,10**-3,10**-2,10**-1,5,10]\n",
        "perf_metric = []\n",
        "for i in tqdm(alpha):\n",
        "    \n",
        "    clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=i, penalty='l1', random_state=42))\n",
        "    clf.fit(x_train_multilabel, y_train)\n",
        "    predictions = clf.predict (x_test_multilabel)\n",
        "    perf_metric.append(f1_score(y_test, predictions, average='micro'))\n",
        "    \n",
        "#print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 7/7 [3:24:36<00:00, 1753.72s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACiZR08XMMSU",
        "colab_type": "code",
        "colab": {},
        "outputId": "115901a8-ecb7-4fcb-a483-62deff13e781"
      },
      "source": [
        "# plot the perf metric for each hyperparam(alpha)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(perf_metric)\n",
        "xlabel = list(range(-11, -3))\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.title(\"Perf-metric vs hyperparam plot\")\n",
        "plt.xlabel(\"Alpha(in 10^)\")\n",
        "plt.ylabel(\"Micro-averaged F1 score\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXpR3_iPMMSa",
        "colab_type": "text"
      },
      "source": [
        "<h1>Training the model with best hyperparameter</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVz_0hssMMSc",
        "colab_type": "code",
        "colab": {},
        "outputId": "841603e9-3a5d-4d86-f2e6-e61d499d792c"
      },
      "source": [
        "start = datetime.now()\n",
        "# fetching the best alpha\n",
        "best_alpha = alpha[np.argmax(perf_metric)]\n",
        "print('Best hyperparam(alpha) : ',best_alpha)\n",
        "\n",
        "# train the LR model with the best alpha\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=best_alpha, penalty='l1',  random_state=42), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict (x_test_multilabel)\n",
        "\n",
        "# print the various performance metrices\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"\\nMicro-average quality numbers -\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"\\nMacro-average quality numbers -\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print(\"\\n\")\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best hyperparam(alpha) :  0.001\n",
            "Accuracy : 0.1366\n",
            "Hamming loss : 0.0044873\n",
            "\n",
            "Micro-average quality numbers -\n",
            "Precision: 0.5368, Recall: 0.3521, F1-measure: 0.4253\n",
            "\n",
            "Macro-average quality numbers -\n",
            "Precision: 0.3906, Recall: 0.2561, F1-measure: 0.2861\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.44      0.56      1805\n",
            "           1       0.83      0.47      0.60      1186\n",
            "           2       0.74      0.56      0.64       484\n",
            "           3       0.81      0.43      0.56      1323\n",
            "           4       0.86      0.59      0.70       739\n",
            "           5       0.88      0.47      0.62      1023\n",
            "           6       0.67      0.41      0.51      1421\n",
            "           7       0.84      0.64      0.73      1450\n",
            "           8       0.92      0.57      0.71      1368\n",
            "           9       0.55      0.41      0.47       914\n",
            "          10       0.59      0.46      0.52       186\n",
            "          11       0.73      0.50      0.59       553\n",
            "          12       0.73      0.39      0.51       644\n",
            "          13       0.46      0.16      0.24       424\n",
            "          14       0.51      0.56      0.53        36\n",
            "          15       0.43      0.43      0.43       352\n",
            "          16       0.49      0.27      0.34       437\n",
            "          17       0.62      0.42      0.50       435\n",
            "          18       0.60      0.42      0.50       153\n",
            "          19       0.89      0.61      0.73       727\n",
            "          20       0.46      0.18      0.25       488\n",
            "          21       0.71      0.37      0.49       272\n",
            "          22       0.77      0.67      0.71       530\n",
            "          23       0.89      0.55      0.68       618\n",
            "          24       0.89      0.54      0.67       614\n",
            "          25       0.56      0.23      0.33       231\n",
            "          26       0.60      0.25      0.35       588\n",
            "          27       0.13      0.21      0.16      1224\n",
            "          28       0.68      0.41      0.51       165\n",
            "          29       0.39      0.60      0.47       231\n",
            "          30       0.56      0.26      0.36       190\n",
            "          31       0.72      0.70      0.71       296\n",
            "          32       0.59      0.35      0.44       274\n",
            "          33       0.50      0.36      0.42       292\n",
            "          34       0.66      0.34      0.45       190\n",
            "          35       0.58      0.25      0.35        99\n",
            "          36       0.86      0.55      0.67       357\n",
            "          37       0.11      0.12      0.12       870\n",
            "          38       0.78      0.45      0.57       135\n",
            "          39       0.60      0.53      0.56        17\n",
            "          40       0.55      0.06      0.11        99\n",
            "          41       0.63      0.29      0.40       176\n",
            "          42       0.16      0.11      0.13       236\n",
            "          43       0.56      0.41      0.47        22\n",
            "          44       0.59      0.23      0.33       106\n",
            "          45       0.15      0.10      0.12       178\n",
            "          46       0.31      0.27      0.29       241\n",
            "          47       0.51      0.18      0.27       217\n",
            "          48       0.58      0.48      0.52       223\n",
            "          49       0.50      0.04      0.07        54\n",
            "          50       0.52      0.33      0.40        92\n",
            "          51       0.76      0.50      0.60       203\n",
            "          52       0.34      0.47      0.39       116\n",
            "          53       0.62      0.54      0.58        72\n",
            "          54       0.25      0.27      0.26        15\n",
            "          55       0.00      0.00      0.00        60\n",
            "          56       0.83      0.88      0.85       216\n",
            "          57       0.21      0.12      0.15        74\n",
            "          58       0.23      0.18      0.20       139\n",
            "          59       0.53      0.44      0.48        91\n",
            "          60       0.43      0.12      0.19       156\n",
            "          61       0.44      0.21      0.29        76\n",
            "          62       0.33      0.25      0.28        89\n",
            "          63       0.41      0.15      0.22       173\n",
            "          64       0.38      0.44      0.41       227\n",
            "          65       0.30      0.19      0.23       383\n",
            "          66       0.46      0.18      0.25       148\n",
            "          67       0.50      0.30      0.38       189\n",
            "          68       0.62      0.20      0.30       169\n",
            "          69       0.14      0.18      0.16        50\n",
            "          70       0.60      0.27      0.37       145\n",
            "          71       0.23      0.39      0.29        31\n",
            "          72       0.84      0.82      0.83       141\n",
            "          73       0.66      0.49      0.56       246\n",
            "          74       0.43      0.29      0.35       210\n",
            "          75       0.85      0.07      0.13       159\n",
            "          76       0.45      0.30      0.36       108\n",
            "          77       0.41      0.75      0.53        65\n",
            "          78       0.64      0.77      0.70       145\n",
            "          79       0.74      0.71      0.72        41\n",
            "          80       0.51      0.75      0.61       129\n",
            "          81       0.68      0.61      0.64        76\n",
            "          82       0.48      0.48      0.48       124\n",
            "          83       0.21      0.23      0.22        69\n",
            "          84       0.26      0.21      0.23        91\n",
            "          85       0.40      0.56      0.47        66\n",
            "          86       0.22      0.21      0.22       100\n",
            "          87       0.29      0.05      0.09        38\n",
            "          88       0.53      0.52      0.53        98\n",
            "          89       0.35      0.18      0.24        38\n",
            "          90       0.87      0.74      0.80       154\n",
            "          91       0.82      0.65      0.73       152\n",
            "          92       0.00      0.00      0.00        13\n",
            "          93       0.00      0.00      0.00        47\n",
            "          94       0.81      0.39      0.52        44\n",
            "          95       0.69      0.40      0.51       200\n",
            "          96       0.31      0.16      0.21        25\n",
            "          97       0.42      0.21      0.28        39\n",
            "          98       0.31      0.37      0.34        51\n",
            "          99       0.21      0.19      0.20        43\n",
            "         100       0.17      0.09      0.12       211\n",
            "         101       0.31      0.28      0.29        18\n",
            "         102       0.55      0.34      0.42        32\n",
            "         103       0.71      0.50      0.59        24\n",
            "         104       0.26      0.36      0.30        14\n",
            "         105       0.50      0.28      0.36        96\n",
            "         106       0.36      0.47      0.41        32\n",
            "         107       0.57      0.50      0.53        80\n",
            "         108       0.08      0.01      0.01       160\n",
            "         109       0.24      0.07      0.11       123\n",
            "         110       0.16      0.01      0.03       202\n",
            "         111       0.55      0.56      0.56        39\n",
            "         112       0.19      0.07      0.11       123\n",
            "         113       0.61      0.62      0.61        55\n",
            "         114       0.25      0.19      0.22        98\n",
            "         115       0.32      0.26      0.29        50\n",
            "         116       0.80      0.48      0.60       275\n",
            "         117       0.00      0.00      0.00       101\n",
            "         118       0.40      0.12      0.18        50\n",
            "         119       0.20      0.24      0.22        41\n",
            "         120       0.44      0.38      0.41        98\n",
            "         121       0.31      0.13      0.19        30\n",
            "         122       0.73      0.33      0.45        73\n",
            "         123       0.84      0.81      0.82       121\n",
            "         124       0.35      0.21      0.26        29\n",
            "         125       1.00      0.09      0.16        57\n",
            "         126       0.21      0.15      0.17        48\n",
            "         127       0.50      0.62      0.56        24\n",
            "         128       0.68      0.27      0.39        48\n",
            "         129       0.50      0.25      0.33        48\n",
            "         130       0.82      0.45      0.58        99\n",
            "         131       0.34      0.38      0.36        29\n",
            "         132       0.15      0.08      0.11        60\n",
            "         133       0.56      0.75      0.64        89\n",
            "         134       0.08      0.02      0.03       113\n",
            "         135       0.23      0.24      0.24        70\n",
            "         136       0.21      0.06      0.09        68\n",
            "         137       0.84      0.60      0.70       146\n",
            "         138       0.43      0.48      0.45        66\n",
            "         139       0.24      0.24      0.24        49\n",
            "         140       0.66      0.65      0.65        51\n",
            "         141       0.75      0.22      0.34        27\n",
            "         142       0.21      0.07      0.11        54\n",
            "         143       0.33      0.14      0.20        21\n",
            "         144       0.29      0.28      0.29        43\n",
            "         145       0.89      0.35      0.50        49\n",
            "         146       0.56      0.36      0.44       137\n",
            "         147       0.66      0.41      0.50        91\n",
            "         148       0.27      0.24      0.25        29\n",
            "         149       0.86      0.49      0.62        88\n",
            "         150       0.04      0.03      0.04        67\n",
            "         151       0.76      0.35      0.48        46\n",
            "         152       0.45      0.26      0.33       187\n",
            "         153       0.73      0.45      0.56        60\n",
            "         154       0.31      0.45      0.36        40\n",
            "         155       0.29      0.06      0.10        67\n",
            "         156       0.21      0.30      0.25        46\n",
            "         157       0.33      0.04      0.08        23\n",
            "         158       0.55      0.59      0.57        54\n",
            "         159       0.34      0.22      0.27        87\n",
            "         160       0.59      0.24      0.34        66\n",
            "         161       0.62      0.45      0.52        69\n",
            "         162       0.29      0.19      0.23        78\n",
            "         163       0.55      0.84      0.67        50\n",
            "         164       0.47      0.06      0.11       115\n",
            "         165       0.39      0.13      0.19        71\n",
            "         166       0.07      0.02      0.04        81\n",
            "         167       0.37      0.44      0.40        52\n",
            "         168       0.47      0.36      0.41        22\n",
            "         169       0.00      0.00      0.00       292\n",
            "         170       0.32      0.56      0.41        45\n",
            "         171       0.14      0.02      0.04       146\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.50      0.23      0.31        66\n",
            "         174       0.04      0.05      0.05        21\n",
            "         175       0.27      0.15      0.20        26\n",
            "         176       0.38      0.10      0.16        86\n",
            "         177       0.50      0.11      0.18        18\n",
            "         178       0.09      0.07      0.08        27\n",
            "         179       0.00      0.00      0.00         0\n",
            "         180       0.06      0.29      0.10         7\n",
            "         181       0.76      0.56      0.64        34\n",
            "         182       0.67      0.57      0.62        35\n",
            "         183       0.51      0.57      0.54        51\n",
            "         184       0.71      0.58      0.64        38\n",
            "         185       0.08      0.03      0.04        39\n",
            "         186       0.00      0.00      0.00        13\n",
            "         187       0.50      0.17      0.26        35\n",
            "         188       0.12      0.16      0.14        44\n",
            "         189       0.16      0.13      0.14        46\n",
            "         190       0.21      0.08      0.11        52\n",
            "         191       0.33      0.19      0.24        88\n",
            "         192       0.11      0.02      0.04        41\n",
            "         193       0.94      0.57      0.71        88\n",
            "         194       0.43      0.06      0.10        51\n",
            "         195       0.47      0.13      0.21       127\n",
            "         196       0.17      0.13      0.15        60\n",
            "         197       0.00      0.00      0.00        18\n",
            "         198       0.10      0.03      0.04        36\n",
            "         199       0.40      0.02      0.04        85\n",
            "         200       0.41      0.27      0.33        48\n",
            "         201       0.44      0.47      0.46        17\n",
            "         202       0.24      0.22      0.23        27\n",
            "         203       0.45      0.22      0.29        60\n",
            "         204       0.46      0.38      0.42       105\n",
            "         205       0.69      0.62      0.65        50\n",
            "         206       0.51      0.42      0.46        45\n",
            "         207       0.17      0.37      0.24        19\n",
            "         208       0.54      0.29      0.38        73\n",
            "         209       0.12      0.02      0.03        51\n",
            "         210       0.00      0.00      0.00        20\n",
            "         211       0.00      0.00      0.00        47\n",
            "         212       0.20      0.05      0.07        44\n",
            "         213       0.37      0.21      0.26        34\n",
            "         214       0.67      0.53      0.59       106\n",
            "         215       0.60      0.10      0.17        59\n",
            "         216       0.09      0.08      0.09        87\n",
            "         217       0.33      0.06      0.11        31\n",
            "         218       0.66      0.67      0.67        46\n",
            "         219       0.20      0.15      0.17        27\n",
            "         220       0.32      0.15      0.21        39\n",
            "         221       0.41      0.13      0.19        55\n",
            "         222       0.67      0.06      0.11        34\n",
            "         223       0.11      0.64      0.19        11\n",
            "         224       0.10      0.14      0.12        51\n",
            "         225       0.09      0.13      0.10        46\n",
            "         226       0.17      0.09      0.11        47\n",
            "         227       0.06      0.07      0.07        14\n",
            "         228       0.33      0.10      0.15        21\n",
            "         229       0.31      0.07      0.12        67\n",
            "         230       0.00      0.00      0.00       229\n",
            "         231       0.30      0.11      0.16        54\n",
            "         232       0.50      0.03      0.06        98\n",
            "         233       0.91      0.40      0.55        53\n",
            "         234       0.62      0.28      0.38        36\n",
            "         235       0.70      0.40      0.51        53\n",
            "         236       0.41      0.37      0.39        68\n",
            "         237       0.08      0.21      0.12        38\n",
            "         238       0.07      0.07      0.07       102\n",
            "         239       0.07      0.33      0.11         6\n",
            "         240       0.15      0.40      0.22         5\n",
            "         241       0.00      0.00      0.00         3\n",
            "         242       0.03      0.03      0.03        68\n",
            "         243       0.38      0.32      0.35        91\n",
            "         244       0.96      0.77      0.85        30\n",
            "         245       0.50      0.10      0.17        50\n",
            "         246       0.00      0.00      0.00         4\n",
            "         247       0.42      0.37      0.39        41\n",
            "         248       0.60      0.33      0.42        98\n",
            "         249       0.00      0.00      0.00         0\n",
            "         250       1.00      1.00      1.00         1\n",
            "         251       0.67      0.15      0.25        26\n",
            "         252       0.56      0.29      0.38        66\n",
            "         253       0.77      0.61      0.68        67\n",
            "         254       0.12      0.06      0.08        32\n",
            "         255       0.00      0.00      0.00         2\n",
            "         256       0.25      0.06      0.10        32\n",
            "         257       0.25      0.25      0.25         4\n",
            "         258       0.14      0.03      0.04        39\n",
            "         259       0.80      0.44      0.57        73\n",
            "         260       0.90      0.64      0.74        55\n",
            "         261       0.20      0.58      0.30        12\n",
            "         262       0.14      0.15      0.14        41\n",
            "         263       0.50      0.07      0.12        14\n",
            "         264       0.55      0.11      0.18        56\n",
            "         265       0.73      0.29      0.41        77\n",
            "         266       0.00      0.00      0.00        13\n",
            "         267       0.33      0.25      0.29        16\n",
            "         268       0.00      0.00      0.00        34\n",
            "         269       0.00      0.00      0.00        45\n",
            "         270       0.06      0.02      0.03        43\n",
            "         271       0.31      0.30      0.31        56\n",
            "         272       0.60      0.27      0.37        11\n",
            "         273       0.03      0.02      0.03        42\n",
            "         274       0.75      0.51      0.61        35\n",
            "         275       0.18      0.15      0.17        59\n",
            "         276       0.06      0.06      0.06        49\n",
            "         277       0.63      0.61      0.62        44\n",
            "         278       0.16      0.07      0.09        46\n",
            "         279       0.06      0.14      0.08         7\n",
            "         280       0.83      0.52      0.64        58\n",
            "         281       0.35      0.20      0.25        46\n",
            "         282       0.42      0.50      0.45        10\n",
            "         283       0.55      0.29      0.37        21\n",
            "         284       0.13      0.13      0.13        47\n",
            "         285       0.47      0.30      0.37        23\n",
            "         286       0.80      0.75      0.77        48\n",
            "         287       0.24      0.17      0.20        35\n",
            "         288       0.00      0.00      0.00        81\n",
            "         289       0.63      0.36      0.46        47\n",
            "         290       0.76      0.74      0.75        93\n",
            "         291       0.00      0.00      0.00        61\n",
            "         292       0.48      0.52      0.50        23\n",
            "         293       0.71      0.50      0.59        10\n",
            "         294       0.25      0.03      0.06        30\n",
            "         295       0.00      0.00      0.00        24\n",
            "         296       0.00      0.00      0.00        54\n",
            "         297       0.35      0.24      0.28        34\n",
            "         298       0.26      0.22      0.24        69\n",
            "         299       0.79      0.68      0.73        44\n",
            "         300       0.60      0.23      0.33        13\n",
            "         301       0.80      0.47      0.59        68\n",
            "         302       0.00      0.00      0.00        33\n",
            "         303       0.75      0.33      0.46        18\n",
            "         304       0.14      0.08      0.10        13\n",
            "         305       0.52      0.25      0.33        53\n",
            "         306       0.41      0.21      0.28        75\n",
            "         307       0.77      0.49      0.60        55\n",
            "         308       0.86      0.51      0.64        61\n",
            "         309       0.70      0.37      0.48        90\n",
            "         310       0.00      0.00      0.00        58\n",
            "         311       0.83      0.79      0.81        19\n",
            "         312       0.44      0.12      0.19        34\n",
            "         313       0.21      0.46      0.29        13\n",
            "         314       0.14      0.25      0.18         4\n",
            "         315       0.20      0.02      0.04        41\n",
            "         316       0.72      0.48      0.58        54\n",
            "         317       0.33      0.04      0.07        25\n",
            "         318       0.14      0.25      0.18         4\n",
            "         319       0.20      0.10      0.14        29\n",
            "         320       0.80      0.22      0.34        37\n",
            "         321       1.00      0.50      0.67         6\n",
            "         322       0.17      0.23      0.20        22\n",
            "         323       0.29      0.11      0.15        19\n",
            "         324       0.14      0.25      0.18         4\n",
            "         325       0.86      0.33      0.48        18\n",
            "         326       0.69      0.43      0.53        21\n",
            "         327       0.00      0.00      0.00        26\n",
            "         328       0.70      0.43      0.53        49\n",
            "         329       0.40      0.51      0.45        35\n",
            "         330       0.00      0.00      0.00        19\n",
            "         331       1.00      0.07      0.12        15\n",
            "         332       0.00      0.00      0.00        10\n",
            "         333       0.68      0.55      0.61        38\n",
            "         334       0.08      0.11      0.10         9\n",
            "         335       0.83      0.09      0.17        53\n",
            "         336       0.83      0.47      0.60        32\n",
            "         337       0.05      0.12      0.07        24\n",
            "         338       0.17      0.33      0.22         3\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         0\n",
            "         341       0.00      0.00      0.00        11\n",
            "         342       0.54      0.55      0.54        40\n",
            "         343       0.21      0.10      0.14        30\n",
            "         344       0.25      0.08      0.12        24\n",
            "         345       0.17      0.30      0.22        23\n",
            "         346       0.38      0.22      0.28        69\n",
            "         347       0.03      0.06      0.04        18\n",
            "         348       0.04      0.03      0.03        65\n",
            "         349       0.43      0.38      0.41        78\n",
            "         350       1.00      0.08      0.15        12\n",
            "         351       0.14      0.08      0.10        13\n",
            "         352       0.38      0.28      0.32        18\n",
            "         353       1.00      0.54      0.70        46\n",
            "         354       0.62      0.40      0.48        40\n",
            "         355       0.00      0.00      0.00        19\n",
            "         356       0.00      0.00      0.00        26\n",
            "         357       0.38      0.08      0.13        39\n",
            "         358       1.00      0.17      0.29        12\n",
            "         359       0.00      0.00      0.00        16\n",
            "         360       0.22      0.08      0.12        24\n",
            "         361       0.22      0.11      0.14        57\n",
            "         362       0.67      0.90      0.77        20\n",
            "         363       0.00      0.00      0.00        84\n",
            "         364       0.60      0.46      0.52        54\n",
            "         365       0.30      0.09      0.14        33\n",
            "         366       0.03      0.03      0.03        30\n",
            "         367       0.00      0.00      0.00        30\n",
            "         368       0.17      0.05      0.08        19\n",
            "         369       0.00      0.00      0.00        19\n",
            "         370       0.33      0.03      0.06        32\n",
            "         371       0.40      0.67      0.50        12\n",
            "         372       0.00      0.00      0.00        15\n",
            "         373       0.03      0.07      0.05        15\n",
            "         374       0.83      0.59      0.69        17\n",
            "         375       0.83      0.61      0.70        41\n",
            "         376       0.86      0.41      0.56        29\n",
            "         377       0.00      0.00      0.00        28\n",
            "         378       0.50      0.21      0.30        19\n",
            "         379       0.06      0.03      0.04        31\n",
            "         380       0.00      0.00      0.00        29\n",
            "         381       0.13      0.18      0.15        49\n",
            "         382       0.00      0.00      0.00         8\n",
            "         383       0.38      0.12      0.19        24\n",
            "         384       0.33      0.20      0.25        20\n",
            "         385       0.38      0.20      0.26        15\n",
            "         386       0.64      0.49      0.55        37\n",
            "         387       0.00      0.00      0.00        22\n",
            "         388       0.00      0.00      0.00        27\n",
            "         389       0.16      0.17      0.16        29\n",
            "         390       0.17      0.10      0.12        20\n",
            "         391       0.54      0.33      0.41        39\n",
            "         392       0.00      0.00      0.00        10\n",
            "         393       1.00      0.05      0.09        42\n",
            "         394       0.11      0.04      0.06        46\n",
            "         395       0.20      0.30      0.24        10\n",
            "         396       1.00      0.05      0.10        39\n",
            "         397       0.00      0.00      0.00        43\n",
            "         398       0.30      0.20      0.24        50\n",
            "         399       0.33      0.29      0.31         7\n",
            "         400       0.00      0.00      0.00        17\n",
            "         401       1.00      0.17      0.29         6\n",
            "         402       0.00      0.00      0.00        26\n",
            "         403       0.04      0.10      0.06        10\n",
            "         404       0.62      0.36      0.45        14\n",
            "         405       0.11      0.07      0.09        14\n",
            "         406       0.80      0.36      0.50        22\n",
            "         407       0.39      0.12      0.18        60\n",
            "         408       0.15      0.10      0.12        40\n",
            "         409       0.00      0.00      0.00        31\n",
            "         410       0.25      0.22      0.24         9\n",
            "         411       0.62      0.26      0.37        19\n",
            "         412       0.69      0.58      0.63        19\n",
            "         413       1.00      0.20      0.33         5\n",
            "         414       0.17      0.08      0.11        12\n",
            "         415       0.86      0.62      0.72        29\n",
            "         416       0.13      0.15      0.14        33\n",
            "         417       0.00      0.00      0.00        33\n",
            "         418       0.08      0.08      0.08        12\n",
            "         419       0.05      0.02      0.03        42\n",
            "         420       0.33      0.42      0.37        12\n",
            "         421       0.00      0.00      0.00        98\n",
            "         422       0.00      0.00      0.00         8\n",
            "         423       0.75      0.43      0.55         7\n",
            "         424       0.50      0.38      0.43        13\n",
            "         425       0.03      0.08      0.04        13\n",
            "         426       0.00      0.00      0.00        20\n",
            "         427       0.00      0.00      0.00        58\n",
            "         428       0.67      1.00      0.80         2\n",
            "         429       0.29      0.26      0.27        27\n",
            "         430       0.49      0.50      0.49        38\n",
            "         431       0.39      0.30      0.34        40\n",
            "         432       0.00      0.00      0.00        43\n",
            "         433       0.96      0.52      0.68        42\n",
            "         434       0.50      0.33      0.40        24\n",
            "         435       0.25      0.03      0.06        31\n",
            "         436       0.33      0.30      0.32        30\n",
            "         437       0.00      0.00      0.00        16\n",
            "         438       0.56      0.45      0.50        22\n",
            "         439       0.00      0.00      0.00         1\n",
            "         440       0.06      0.05      0.06        19\n",
            "         441       0.25      0.22      0.24         9\n",
            "         442       0.00      0.00      0.00       100\n",
            "         443       0.50      0.32      0.39        28\n",
            "         444       0.60      0.60      0.60        20\n",
            "         445       0.44      0.41      0.43        29\n",
            "         446       0.17      0.05      0.07        21\n",
            "         447       0.00      0.00      0.00        20\n",
            "         448       0.85      0.29      0.43        38\n",
            "         449       0.00      0.00      0.00        22\n",
            "         450       0.56      0.48      0.51        21\n",
            "         451       0.00      0.00      0.00        13\n",
            "         452       0.00      0.00      0.00        24\n",
            "         453       0.40      0.04      0.08        48\n",
            "         454       0.00      0.00      0.00        75\n",
            "         455       0.00      0.00      0.00        18\n",
            "         456       0.00      0.00      0.00         3\n",
            "         457       0.32      0.46      0.37        13\n",
            "         458       0.00      0.00      0.00        13\n",
            "         459       0.06      0.04      0.05        24\n",
            "         460       0.27      0.17      0.21        36\n",
            "         461       0.20      0.11      0.14        18\n",
            "         462       0.50      0.03      0.06        31\n",
            "         463       0.00      0.00      0.00        28\n",
            "         464       0.25      0.14      0.18         7\n",
            "         465       0.80      0.30      0.43        27\n",
            "         466       0.00      0.00      0.00        12\n",
            "         467       0.00      0.00      0.00        14\n",
            "         468       0.00      0.00      0.00         6\n",
            "         469       0.25      0.18      0.21        17\n",
            "         470       0.15      0.22      0.18        18\n",
            "         471       0.02      0.03      0.03        29\n",
            "         472       0.00      0.00      0.00         2\n",
            "         473       0.33      0.03      0.05        34\n",
            "         474       0.00      0.00      0.00         8\n",
            "         475       1.00      0.25      0.40         4\n",
            "         476       0.22      0.09      0.13        22\n",
            "         477       0.20      0.50      0.29         6\n",
            "         478       0.26      0.29      0.28        17\n",
            "         479       0.20      0.04      0.07        23\n",
            "         480       1.00      0.06      0.11        18\n",
            "         481       0.67      0.18      0.29        11\n",
            "         482       0.93      0.37      0.53        35\n",
            "         483       0.61      0.52      0.56        21\n",
            "         484       0.83      0.54      0.65        28\n",
            "         485       0.33      0.29      0.31        14\n",
            "         486       0.91      0.91      0.91        11\n",
            "         487       1.00      0.20      0.33        15\n",
            "         488       0.00      0.00      0.00        38\n",
            "         489       0.00      0.00      0.00        75\n",
            "         490       1.00      0.12      0.21        51\n",
            "         491       1.00      0.58      0.73        19\n",
            "         492       0.45      0.24      0.31        21\n",
            "         493       0.00      0.00      0.00        16\n",
            "         494       0.44      0.67      0.53         6\n",
            "         495       0.11      0.09      0.10        22\n",
            "         496       0.49      0.49      0.49        37\n",
            "         497       0.17      0.10      0.12        20\n",
            "         498       0.56      0.42      0.48        24\n",
            "         499       0.00      0.00      0.00        17\n",
            "\n",
            "   micro avg       0.54      0.35      0.43     47151\n",
            "   macro avg       0.39      0.26      0.29     47151\n",
            "weighted avg       0.55      0.35      0.41     47151\n",
            " samples avg       0.42      0.34      0.35     47151\n",
            "\n",
            "Time taken to run this cell : 0:03:22.300054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-rU1csMMSm",
        "colab_type": "text"
      },
      "source": [
        "<h1>Task 3: Apply OneVsRestClassifier with Linear-SVM </a>  </h1>\n",
        "    <h3>      Hyperparameter Tuning </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXbtv9G0MMSo",
        "colab_type": "code",
        "colab": {},
        "outputId": "088ba353-37e8-4f18-89b0-3010146b754f"
      },
      "source": [
        "from tqdm import tqdm\n",
        "start = datetime.now()\n",
        "alpha = [10 ** x for x in range(-10, -3, 2)]\n",
        "perf_metric = []\n",
        "for i in tqdm(alpha):\n",
        "    clf = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=i, penalty='l1', random_state=42), n_jobs=-1)\n",
        "    clf.fit(x_train_multilabel, y_train)\n",
        "    predictions = clf.predict (x_test_multilabel)\n",
        "    # append the micro-f1 score for the particular alpha trained classifier\n",
        "    perf_metric.append(f1_score(y_test, predictions, average='micro'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [29:51<00:00, 447.80s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:29:51.230015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTW0ZhM2MMSu",
        "colab_type": "code",
        "colab": {},
        "outputId": "01bb0c2c-29bb-4d34-eb73-2cb1dea3fdda"
      },
      "source": [
        "# plot the perf metric for each hyperparam(alpha)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(perf_metric)\n",
        "xlabel = list(range(-11, -3))\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.title(\"Perf-metric vs hyperparam plot - Lin SVM\")\n",
        "plt.xlabel(\"Alpha(in 10^)\")\n",
        "plt.ylabel(\"Micro-averaged F1 score\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGohCp8AMMS2",
        "colab_type": "code",
        "colab": {},
        "outputId": "87fc7f24-8cfa-4088-fb7d-1ec12634db6a"
      },
      "source": [
        "start = datetime.now()\n",
        "# fetching the best alpha\n",
        "best_alpha = alpha[np.argmax(perf_metric)]\n",
        "print('Best hyperparam(alpha) : ',best_alpha)\n",
        "\n",
        "# train the Lin SVM model with the best alpha\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=best_alpha, penalty='l1',  random_state=42), n_jobs=-1)\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict (x_test_multilabel)\n",
        "\n",
        "# print the various performance metrices\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"\\nMicro-average quality numbers -\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"\\nMacro-average quality numbers -\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print(\"\\n\")\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best hyperparam(alpha) :  0.0001\n",
            "Accuracy : 0.0877\n",
            "Hamming loss : 0.006592\n",
            "\n",
            "Micro-average quality numbers -\n",
            "Precision: 0.3473, Recall: 0.4527, F1-measure: 0.3931\n",
            "\n",
            "Macro-average quality numbers -\n",
            "Precision: 0.2390, Recall: 0.3426, F1-measure: 0.2656\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.53      0.56      1805\n",
            "           1       0.64      0.59      0.62      1186\n",
            "           2       0.52      0.70      0.60       484\n",
            "           3       0.60      0.51      0.55      1323\n",
            "           4       0.56      0.65      0.60       739\n",
            "           5       0.65      0.50      0.56      1023\n",
            "           6       0.54      0.47      0.50      1421\n",
            "           7       0.75      0.72      0.74      1450\n",
            "           8       0.83      0.80      0.82      1368\n",
            "           9       0.50      0.51      0.51       914\n",
            "          10       0.25      0.51      0.33       186\n",
            "          11       0.57      0.53      0.54       553\n",
            "          12       0.57      0.52      0.54       644\n",
            "          13       0.34      0.33      0.34       424\n",
            "          14       0.11      0.47      0.18        36\n",
            "          15       0.34      0.44      0.38       352\n",
            "          16       0.30      0.38      0.34       437\n",
            "          17       0.42      0.51      0.46       435\n",
            "          18       0.44      0.65      0.53       153\n",
            "          19       0.78      0.70      0.74       727\n",
            "          20       0.32      0.44      0.37       488\n",
            "          21       0.48      0.71      0.58       272\n",
            "          22       0.58      0.71      0.64       530\n",
            "          23       0.80      0.63      0.71       618\n",
            "          24       0.81      0.63      0.70       614\n",
            "          25       0.22      0.44      0.30       231\n",
            "          26       0.30      0.66      0.41       588\n",
            "          27       0.24      0.45      0.31      1224\n",
            "          28       0.54      0.60      0.57       165\n",
            "          29       0.32      0.61      0.42       231\n",
            "          30       0.20      0.42      0.27       190\n",
            "          31       0.63      0.66      0.64       296\n",
            "          32       0.33      0.36      0.35       274\n",
            "          33       0.29      0.42      0.34       292\n",
            "          34       0.31      0.35      0.33       190\n",
            "          35       0.47      0.58      0.52        99\n",
            "          36       0.56      0.65      0.60       357\n",
            "          37       0.25      0.39      0.30       870\n",
            "          38       0.50      0.56      0.53       135\n",
            "          39       0.12      0.47      0.19        17\n",
            "          40       0.17      0.17      0.17        99\n",
            "          41       0.25      0.40      0.31       176\n",
            "          42       0.14      0.19      0.16       236\n",
            "          43       0.09      0.32      0.14        22\n",
            "          44       0.21      0.29      0.25       106\n",
            "          45       0.19      0.24      0.21       178\n",
            "          46       0.18      0.31      0.23       241\n",
            "          47       0.26      0.28      0.27       217\n",
            "          48       0.44      0.54      0.48       223\n",
            "          49       0.09      0.11      0.10        54\n",
            "          50       0.29      0.50      0.37        92\n",
            "          51       0.65      0.51      0.57       203\n",
            "          52       0.29      0.51      0.37       116\n",
            "          53       0.19      0.56      0.29        72\n",
            "          54       0.04      0.20      0.07        15\n",
            "          55       0.17      0.15      0.16        60\n",
            "          56       0.71      0.86      0.78       216\n",
            "          57       0.28      0.19      0.23        74\n",
            "          58       0.12      0.06      0.09       139\n",
            "          59       0.37      0.58      0.45        91\n",
            "          60       0.15      0.19      0.17       156\n",
            "          61       0.31      0.37      0.34        76\n",
            "          62       0.13      0.21      0.16        89\n",
            "          63       0.15      0.21      0.17       173\n",
            "          64       0.37      0.51      0.43       227\n",
            "          65       0.26      0.31      0.28       383\n",
            "          66       0.18      0.32      0.24       148\n",
            "          67       0.44      0.53      0.48       189\n",
            "          68       0.29      0.37      0.33       169\n",
            "          69       0.06      0.22      0.10        50\n",
            "          70       0.24      0.42      0.31       145\n",
            "          71       0.14      0.26      0.18        31\n",
            "          72       0.57      0.82      0.67       141\n",
            "          73       0.39      0.59      0.47       246\n",
            "          74       0.35      0.37      0.36       210\n",
            "          75       0.26      0.20      0.23       159\n",
            "          76       0.19      0.29      0.23       108\n",
            "          77       0.59      0.78      0.67        65\n",
            "          78       0.63      0.72      0.67       145\n",
            "          79       0.53      0.76      0.63        41\n",
            "          80       0.46      0.74      0.56       129\n",
            "          81       0.28      0.63      0.38        76\n",
            "          82       0.28      0.50      0.36       124\n",
            "          83       0.08      0.19      0.11        69\n",
            "          84       0.14      0.21      0.17        91\n",
            "          85       0.20      0.53      0.29        66\n",
            "          86       0.15      0.23      0.18       100\n",
            "          87       0.22      0.34      0.27        38\n",
            "          88       0.34      0.28      0.31        98\n",
            "          89       0.29      0.58      0.38        38\n",
            "          90       0.55      0.73      0.63       154\n",
            "          91       0.49      0.72      0.58       152\n",
            "          92       0.00      0.00      0.00        13\n",
            "          93       0.02      0.02      0.02        47\n",
            "          94       0.14      0.45      0.21        44\n",
            "          95       0.30      0.47      0.36       200\n",
            "          96       0.21      0.28      0.24        25\n",
            "          97       0.22      0.36      0.27        39\n",
            "          98       0.23      0.45      0.31        51\n",
            "          99       0.13      0.26      0.17        43\n",
            "         100       0.26      0.22      0.24       211\n",
            "         101       0.14      0.33      0.20        18\n",
            "         102       0.37      0.56      0.44        32\n",
            "         103       0.09      0.58      0.15        24\n",
            "         104       0.03      0.21      0.05        14\n",
            "         105       0.42      0.43      0.42        96\n",
            "         106       0.41      0.53      0.47        32\n",
            "         107       0.46      0.34      0.39        80\n",
            "         108       0.42      0.34      0.37       160\n",
            "         109       0.14      0.11      0.12       123\n",
            "         110       0.13      0.22      0.16       202\n",
            "         111       0.34      0.56      0.43        39\n",
            "         112       0.23      0.26      0.24       123\n",
            "         113       0.39      0.51      0.44        55\n",
            "         114       0.16      0.14      0.15        98\n",
            "         115       0.12      0.30      0.18        50\n",
            "         116       0.34      0.57      0.43       275\n",
            "         117       0.08      0.11      0.09       101\n",
            "         118       0.14      0.24      0.18        50\n",
            "         119       0.19      0.27      0.22        41\n",
            "         120       0.33      0.32      0.32        98\n",
            "         121       0.07      0.20      0.10        30\n",
            "         122       0.29      0.49      0.36        73\n",
            "         123       0.49      0.83      0.62       121\n",
            "         124       0.26      0.55      0.36        29\n",
            "         125       0.43      0.37      0.40        57\n",
            "         126       0.14      0.12      0.13        48\n",
            "         127       0.19      0.79      0.30        24\n",
            "         128       0.20      0.23      0.21        48\n",
            "         129       0.24      0.27      0.25        48\n",
            "         130       0.39      0.62      0.48        99\n",
            "         131       0.09      0.41      0.14        29\n",
            "         132       0.08      0.13      0.10        60\n",
            "         133       0.53      0.62      0.57        89\n",
            "         134       0.07      0.11      0.09       113\n",
            "         135       0.12      0.33      0.18        70\n",
            "         136       0.11      0.22      0.15        68\n",
            "         137       0.59      0.62      0.61       146\n",
            "         138       0.30      0.47      0.36        66\n",
            "         139       0.07      0.22      0.11        49\n",
            "         140       0.20      0.59      0.30        51\n",
            "         141       0.26      0.41      0.32        27\n",
            "         142       0.07      0.13      0.09        54\n",
            "         143       0.05      0.14      0.07        21\n",
            "         144       0.15      0.44      0.22        43\n",
            "         145       0.45      0.37      0.40        49\n",
            "         146       0.36      0.49      0.41       137\n",
            "         147       0.37      0.56      0.45        91\n",
            "         148       0.16      0.41      0.24        29\n",
            "         149       0.50      0.62      0.56        88\n",
            "         150       0.11      0.16      0.13        67\n",
            "         151       0.40      0.41      0.40        46\n",
            "         152       0.24      0.32      0.27       187\n",
            "         153       0.27      0.43      0.33        60\n",
            "         154       0.42      0.40      0.41        40\n",
            "         155       0.17      0.06      0.09        67\n",
            "         156       0.18      0.33      0.23        46\n",
            "         157       0.17      0.43      0.25        23\n",
            "         158       0.41      0.59      0.48        54\n",
            "         159       0.22      0.23      0.22        87\n",
            "         160       0.37      0.36      0.37        66\n",
            "         161       0.42      0.58      0.48        69\n",
            "         162       0.15      0.36      0.21        78\n",
            "         163       0.65      0.88      0.75        50\n",
            "         164       0.15      0.23      0.18       115\n",
            "         165       0.39      0.15      0.22        71\n",
            "         166       0.05      0.07      0.06        81\n",
            "         167       0.18      0.50      0.26        52\n",
            "         168       0.28      0.59      0.38        22\n",
            "         169       0.50      0.01      0.01       292\n",
            "         170       0.20      0.58      0.30        45\n",
            "         171       0.08      0.03      0.04       146\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.14      0.05      0.07        66\n",
            "         174       0.08      0.29      0.13        21\n",
            "         175       0.10      0.23      0.14        26\n",
            "         176       0.16      0.14      0.15        86\n",
            "         177       0.09      0.17      0.12        18\n",
            "         178       0.04      0.11      0.06        27\n",
            "         179       0.00      0.00      0.00         0\n",
            "         180       0.11      0.57      0.19         7\n",
            "         181       0.54      0.62      0.58        34\n",
            "         182       0.46      0.66      0.54        35\n",
            "         183       0.35      0.57      0.43        51\n",
            "         184       0.41      0.68      0.51        38\n",
            "         185       0.00      0.00      0.00        39\n",
            "         186       0.25      0.38      0.30        13\n",
            "         187       0.34      0.40      0.37        35\n",
            "         188       0.09      0.18      0.12        44\n",
            "         189       0.12      0.24      0.16        46\n",
            "         190       0.17      0.23      0.20        52\n",
            "         191       0.17      0.23      0.20        88\n",
            "         192       0.04      0.07      0.05        41\n",
            "         193       0.86      0.70      0.78        88\n",
            "         194       0.05      0.10      0.07        51\n",
            "         195       0.28      0.32      0.30       127\n",
            "         196       0.07      0.12      0.09        60\n",
            "         197       0.11      0.22      0.15        18\n",
            "         198       0.04      0.06      0.04        36\n",
            "         199       0.09      0.16      0.11        85\n",
            "         200       0.22      0.31      0.26        48\n",
            "         201       0.17      0.71      0.28        17\n",
            "         202       0.21      0.30      0.24        27\n",
            "         203       0.17      0.45      0.25        60\n",
            "         204       0.44      0.51      0.48       105\n",
            "         205       0.42      0.44      0.43        50\n",
            "         206       0.21      0.33      0.25        45\n",
            "         207       0.28      0.58      0.37        19\n",
            "         208       0.22      0.38      0.28        73\n",
            "         209       0.21      0.12      0.15        51\n",
            "         210       0.15      0.25      0.19        20\n",
            "         211       0.08      0.09      0.08        47\n",
            "         212       0.06      0.05      0.05        44\n",
            "         213       0.32      0.35      0.34        34\n",
            "         214       0.46      0.55      0.50       106\n",
            "         215       0.31      0.47      0.37        59\n",
            "         216       0.12      0.22      0.15        87\n",
            "         217       0.23      0.29      0.25        31\n",
            "         218       0.35      0.72      0.47        46\n",
            "         219       0.04      0.22      0.06        27\n",
            "         220       0.09      0.10      0.10        39\n",
            "         221       0.24      0.35      0.29        55\n",
            "         222       0.40      0.18      0.24        34\n",
            "         223       0.16      0.64      0.26        11\n",
            "         224       0.11      0.14      0.12        51\n",
            "         225       0.07      0.11      0.08        46\n",
            "         226       0.11      0.23      0.15        47\n",
            "         227       0.06      0.14      0.09        14\n",
            "         228       0.12      0.24      0.16        21\n",
            "         229       0.15      0.25      0.18        67\n",
            "         230       0.00      0.00      0.00       229\n",
            "         231       0.09      0.15      0.11        54\n",
            "         232       0.36      0.16      0.22        98\n",
            "         233       0.63      0.45      0.53        53\n",
            "         234       0.19      0.33      0.24        36\n",
            "         235       0.25      0.51      0.33        53\n",
            "         236       0.28      0.40      0.33        68\n",
            "         237       0.05      0.21      0.09        38\n",
            "         238       0.14      0.23      0.17       102\n",
            "         239       0.07      0.33      0.12         6\n",
            "         240       0.03      0.20      0.06         5\n",
            "         241       0.15      0.67      0.25         3\n",
            "         242       0.16      0.16      0.16        68\n",
            "         243       0.38      0.38      0.38        91\n",
            "         244       0.35      0.83      0.49        30\n",
            "         245       0.21      0.32      0.26        50\n",
            "         246       0.06      0.25      0.10         4\n",
            "         247       0.25      0.44      0.32        41\n",
            "         248       0.29      0.26      0.27        98\n",
            "         249       0.00      0.00      0.00         0\n",
            "         250       0.10      1.00      0.18         1\n",
            "         251       0.12      0.27      0.16        26\n",
            "         252       0.42      0.27      0.33        66\n",
            "         253       0.44      0.70      0.54        67\n",
            "         254       0.02      0.06      0.03        32\n",
            "         255       0.00      0.00      0.00         2\n",
            "         256       0.07      0.16      0.10        32\n",
            "         257       0.03      0.50      0.05         4\n",
            "         258       0.04      0.08      0.05        39\n",
            "         259       0.51      0.49      0.50        73\n",
            "         260       0.71      0.55      0.62        55\n",
            "         261       0.24      0.67      0.36        12\n",
            "         262       0.11      0.24      0.15        41\n",
            "         263       0.25      0.29      0.27        14\n",
            "         264       0.15      0.20      0.17        56\n",
            "         265       0.37      0.38      0.37        77\n",
            "         266       0.00      0.00      0.00        13\n",
            "         267       0.27      0.44      0.33        16\n",
            "         268       0.02      0.03      0.03        34\n",
            "         269       0.04      0.02      0.03        45\n",
            "         270       0.06      0.12      0.08        43\n",
            "         271       0.27      0.46      0.34        56\n",
            "         272       0.14      0.27      0.19        11\n",
            "         273       0.10      0.05      0.06        42\n",
            "         274       0.69      0.57      0.62        35\n",
            "         275       0.04      0.03      0.04        59\n",
            "         276       0.07      0.18      0.11        49\n",
            "         277       0.61      0.64      0.62        44\n",
            "         278       0.11      0.11      0.11        46\n",
            "         279       0.00      0.00      0.00         7\n",
            "         280       0.55      0.66      0.60        58\n",
            "         281       0.48      0.26      0.34        46\n",
            "         282       0.19      0.50      0.27        10\n",
            "         283       0.30      0.33      0.32        21\n",
            "         284       0.07      0.11      0.09        47\n",
            "         285       0.15      0.26      0.19        23\n",
            "         286       0.49      0.77      0.60        48\n",
            "         287       0.39      0.51      0.44        35\n",
            "         288       0.05      0.04      0.04        81\n",
            "         289       0.37      0.53      0.44        47\n",
            "         290       0.62      0.83      0.71        93\n",
            "         291       0.18      0.21      0.19        61\n",
            "         292       0.26      0.70      0.38        23\n",
            "         293       0.23      0.50      0.31        10\n",
            "         294       0.22      0.07      0.10        30\n",
            "         295       0.05      0.08      0.06        24\n",
            "         296       0.08      0.07      0.08        54\n",
            "         297       0.23      0.62      0.34        34\n",
            "         298       0.21      0.39      0.28        69\n",
            "         299       0.63      0.86      0.73        44\n",
            "         300       0.47      0.54      0.50        13\n",
            "         301       0.52      0.56      0.54        68\n",
            "         302       0.02      0.06      0.04        33\n",
            "         303       0.28      0.39      0.33        18\n",
            "         304       0.07      0.38      0.11        13\n",
            "         305       0.19      0.28      0.23        53\n",
            "         306       0.22      0.33      0.26        75\n",
            "         307       0.45      0.62      0.52        55\n",
            "         308       0.79      0.67      0.73        61\n",
            "         309       0.46      0.49      0.47        90\n",
            "         310       0.53      0.16      0.24        58\n",
            "         311       0.28      0.84      0.42        19\n",
            "         312       0.16      0.24      0.19        34\n",
            "         313       0.16      0.38      0.22        13\n",
            "         314       0.11      0.50      0.18         4\n",
            "         315       0.06      0.07      0.07        41\n",
            "         316       0.43      0.52      0.47        54\n",
            "         317       0.18      0.24      0.20        25\n",
            "         318       0.13      0.50      0.21         4\n",
            "         319       0.04      0.14      0.06        29\n",
            "         320       0.10      0.16      0.13        37\n",
            "         321       0.33      0.50      0.40         6\n",
            "         322       0.22      0.50      0.30        22\n",
            "         323       0.08      0.11      0.09        19\n",
            "         324       0.12      0.50      0.20         4\n",
            "         325       0.29      0.56      0.38        18\n",
            "         326       0.35      0.57      0.44        21\n",
            "         327       0.07      0.12      0.09        26\n",
            "         328       0.28      0.55      0.37        49\n",
            "         329       0.43      0.51      0.47        35\n",
            "         330       0.00      0.00      0.00        19\n",
            "         331       0.17      0.20      0.18        15\n",
            "         332       0.08      0.20      0.12        10\n",
            "         333       0.51      0.50      0.51        38\n",
            "         334       0.06      0.22      0.09         9\n",
            "         335       0.39      0.21      0.27        53\n",
            "         336       0.68      0.66      0.67        32\n",
            "         337       0.05      0.08      0.06        24\n",
            "         338       0.03      0.33      0.05         3\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         0\n",
            "         341       0.12      0.27      0.17        11\n",
            "         342       0.27      0.72      0.39        40\n",
            "         343       0.12      0.07      0.09        30\n",
            "         344       0.11      0.04      0.06        24\n",
            "         345       0.08      0.35      0.13        23\n",
            "         346       0.29      0.25      0.27        69\n",
            "         347       0.10      0.22      0.14        18\n",
            "         348       0.11      0.20      0.14        65\n",
            "         349       0.33      0.27      0.30        78\n",
            "         350       0.00      0.00      0.00        12\n",
            "         351       0.10      0.46      0.16        13\n",
            "         352       0.00      0.00      0.00        18\n",
            "         353       0.76      0.76      0.76        46\n",
            "         354       0.24      0.47      0.32        40\n",
            "         355       0.05      0.11      0.07        19\n",
            "         356       0.08      0.08      0.08        26\n",
            "         357       0.23      0.23      0.23        39\n",
            "         358       0.15      0.17      0.16        12\n",
            "         359       0.00      0.00      0.00        16\n",
            "         360       0.08      0.08      0.08        24\n",
            "         361       0.20      0.18      0.19        57\n",
            "         362       0.55      0.85      0.67        20\n",
            "         363       0.36      0.11      0.17        84\n",
            "         364       0.44      0.67      0.53        54\n",
            "         365       0.15      0.21      0.18        33\n",
            "         366       0.05      0.10      0.07        30\n",
            "         367       0.08      0.03      0.05        30\n",
            "         368       0.00      0.00      0.00        19\n",
            "         369       0.00      0.00      0.00        19\n",
            "         370       0.05      0.06      0.06        32\n",
            "         371       0.15      0.67      0.25        12\n",
            "         372       0.04      0.07      0.05        15\n",
            "         373       0.05      0.13      0.07        15\n",
            "         374       0.61      0.65      0.63        17\n",
            "         375       0.52      0.83      0.64        41\n",
            "         376       0.28      0.62      0.39        29\n",
            "         377       0.04      0.04      0.04        28\n",
            "         378       0.09      0.16      0.12        19\n",
            "         379       0.16      0.16      0.16        31\n",
            "         380       0.18      0.14      0.16        29\n",
            "         381       0.11      0.22      0.14        49\n",
            "         382       0.03      0.12      0.05         8\n",
            "         383       0.08      0.29      0.13        24\n",
            "         384       0.11      0.45      0.17        20\n",
            "         385       0.06      0.13      0.09        15\n",
            "         386       0.52      0.59      0.56        37\n",
            "         387       0.03      0.09      0.05        22\n",
            "         388       0.00      0.00      0.00        27\n",
            "         389       0.20      0.34      0.25        29\n",
            "         390       0.06      0.10      0.07        20\n",
            "         391       0.33      0.56      0.42        39\n",
            "         392       0.06      0.10      0.07        10\n",
            "         393       0.27      0.31      0.29        42\n",
            "         394       0.12      0.13      0.12        46\n",
            "         395       0.03      0.10      0.05        10\n",
            "         396       0.00      0.00      0.00        39\n",
            "         397       0.00      0.00      0.00        43\n",
            "         398       0.27      0.24      0.25        50\n",
            "         399       0.12      0.71      0.21         7\n",
            "         400       0.02      0.06      0.03        17\n",
            "         401       0.08      0.33      0.13         6\n",
            "         402       0.00      0.00      0.00        26\n",
            "         403       0.02      0.10      0.03        10\n",
            "         404       0.17      0.29      0.22        14\n",
            "         405       0.07      0.14      0.10        14\n",
            "         406       0.50      0.55      0.52        22\n",
            "         407       0.22      0.23      0.22        60\n",
            "         408       0.26      0.28      0.27        40\n",
            "         409       0.06      0.13      0.08        31\n",
            "         410       0.17      0.33      0.22         9\n",
            "         411       0.15      0.32      0.20        19\n",
            "         412       0.41      0.63      0.50        19\n",
            "         413       0.17      0.20      0.18         5\n",
            "         414       0.00      0.08      0.01        12\n",
            "         415       0.47      0.59      0.52        29\n",
            "         416       0.04      0.09      0.06        33\n",
            "         417       0.10      0.21      0.13        33\n",
            "         418       0.04      0.08      0.05        12\n",
            "         419       0.11      0.14      0.12        42\n",
            "         420       0.12      0.08      0.10        12\n",
            "         421       0.29      0.32      0.30        98\n",
            "         422       0.06      0.12      0.08         8\n",
            "         423       0.08      0.14      0.10         7\n",
            "         424       0.25      0.62      0.36        13\n",
            "         425       0.08      0.15      0.11        13\n",
            "         426       0.13      0.30      0.18        20\n",
            "         427       0.07      0.12      0.09        58\n",
            "         428       0.25      1.00      0.40         2\n",
            "         429       0.28      0.41      0.33        27\n",
            "         430       0.35      0.39      0.37        38\n",
            "         431       0.22      0.38      0.28        40\n",
            "         432       0.03      0.12      0.05        43\n",
            "         433       0.60      0.67      0.63        42\n",
            "         434       0.21      0.21      0.21        24\n",
            "         435       0.12      0.23      0.16        31\n",
            "         436       0.17      0.17      0.17        30\n",
            "         437       0.06      0.12      0.08        16\n",
            "         438       0.31      0.41      0.35        22\n",
            "         439       0.00      0.00      0.00         1\n",
            "         440       0.09      0.11      0.10        19\n",
            "         441       0.03      0.22      0.05         9\n",
            "         442       0.30      0.20      0.24       100\n",
            "         443       0.44      0.57      0.50        28\n",
            "         444       0.34      0.75      0.47        20\n",
            "         445       0.41      0.62      0.49        29\n",
            "         446       0.04      0.10      0.06        21\n",
            "         447       0.24      0.45      0.32        20\n",
            "         448       0.78      0.55      0.65        38\n",
            "         449       0.05      0.09      0.06        22\n",
            "         450       0.45      0.43      0.44        21\n",
            "         451       0.07      0.23      0.11        13\n",
            "         452       0.03      0.08      0.05        24\n",
            "         453       0.20      0.10      0.14        48\n",
            "         454       0.28      0.32      0.30        75\n",
            "         455       0.14      0.22      0.17        18\n",
            "         456       0.04      0.33      0.07         3\n",
            "         457       0.19      0.46      0.27        13\n",
            "         458       0.00      0.00      0.00        13\n",
            "         459       0.18      0.33      0.24        24\n",
            "         460       0.20      0.36      0.25        36\n",
            "         461       0.31      0.61      0.42        18\n",
            "         462       0.18      0.32      0.23        31\n",
            "         463       0.22      0.14      0.17        28\n",
            "         464       0.00      0.00      0.00         7\n",
            "         465       0.26      0.33      0.30        27\n",
            "         466       0.41      0.75      0.53        12\n",
            "         467       0.12      0.21      0.15        14\n",
            "         468       0.00      0.00      0.00         6\n",
            "         469       0.08      0.12      0.10        17\n",
            "         470       0.11      0.28      0.16        18\n",
            "         471       0.14      0.10      0.12        29\n",
            "         472       0.00      0.00      0.00         2\n",
            "         473       0.35      0.18      0.24        34\n",
            "         474       0.00      0.00      0.00         8\n",
            "         475       0.10      0.50      0.17         4\n",
            "         476       0.28      0.50      0.35        22\n",
            "         477       0.09      0.67      0.15         6\n",
            "         478       0.20      0.24      0.22        17\n",
            "         479       0.25      0.04      0.07        23\n",
            "         480       0.18      0.28      0.22        18\n",
            "         481       0.04      0.18      0.06        11\n",
            "         482       0.32      0.34      0.33        35\n",
            "         483       0.31      0.57      0.40        21\n",
            "         484       0.47      0.61      0.53        28\n",
            "         485       0.11      0.21      0.15        14\n",
            "         486       0.42      0.73      0.53        11\n",
            "         487       0.21      0.33      0.26        15\n",
            "         488       0.16      0.21      0.18        38\n",
            "         489       0.15      0.16      0.15        75\n",
            "         490       0.89      0.49      0.63        51\n",
            "         491       0.74      0.74      0.74        19\n",
            "         492       0.25      0.24      0.24        21\n",
            "         493       0.07      0.25      0.11        16\n",
            "         494       0.31      0.83      0.45         6\n",
            "         495       0.05      0.05      0.05        22\n",
            "         496       0.27      0.49      0.35        37\n",
            "         497       0.03      0.05      0.04        20\n",
            "         498       0.55      0.50      0.52        24\n",
            "         499       0.04      0.12      0.06        17\n",
            "\n",
            "   micro avg       0.35      0.45      0.39     47151\n",
            "   macro avg       0.24      0.34      0.27     47151\n",
            "weighted avg       0.40      0.45      0.41     47151\n",
            " samples avg       0.39      0.44      0.37     47151\n",
            "\n",
            "Time taken to run this cell : 0:05:57.704523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYGlZz9UMMS9",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5c03e53-0e97-44f8-afca-a7845322332e"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "tb = PrettyTable()\n",
        "tb.field_names= (\"Vectorizer\",                        \"Model\",                                                          \"  Micro Averaged F1 Score\")\n",
        "tb.add_row([\"                tf-idf\",                   \"Logistic Regression with OVR classifier\",0.5005])\n",
        "tb.add_row([\"                Bow\",                   \"Logistic Regression with OVR classifier\",     0.498])\n",
        "tb.add_row([\"                Bow\",                    \"SGD classifier(Logistic loss) with OVR classifier with parameter tuning\",0.4995])\n",
        "tb.add_row([\"                Bow\",                     \"SGD classifier(Hinge loss)  with OVR classifier with parameter tuning\", 0.3931])\n",
        "print(tb.get_string(titles = \"KNN - Observations\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-------------------------------------------------------------------------+---------------------------+\n",
            "|       Vectorizer       |                                  Model                                  |   Micro Averaged F1 Score |\n",
            "+------------------------+-------------------------------------------------------------------------+---------------------------+\n",
            "|                 tf-idf |                 Logistic Regression with OVR classifier                 |           0.5005          |\n",
            "|                  Bow   |                 Logistic Regression with OVR classifier                 |           0.498           |\n",
            "|                  Bow   | SGD classifier(Logistic loss) with OVR classifier with parameter tuning |           0.4995          |\n",
            "|                  Bow   |  SGD classifier(Hinge loss)  with OVR classifier with parameter tuning  |           0.3931          |\n",
            "+------------------------+-------------------------------------------------------------------------+---------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqBxQOBAMMTG",
        "colab_type": "text"
      },
      "source": [
        "# Step by Step Procedure\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuZUoHAfMMTH",
        "colab_type": "text"
      },
      "source": [
        "- Get the Data from csv file and load into the sqlite database.\n",
        "- Remove the duplicates rows and load the data in a new database.\n",
        "- Analysis on tags and save the dictionary(Frequecny of each tag) into csv file.\n",
        "- Text preprocessing and save the preprocessed text  in a new database.\n",
        "- Now we have 42k tags, now we will reduce the unnecessary tags and use only the most frequent 5500 tags that covered 99.08% questions.\n",
        "- Now we have many rows, high dimensions  with 5500 tags, even if we apply a simple logistic regression with one vs rest classifier it'll take above24 hours with my low ram.\n",
        "- Now i Took a 0.1 million datapoint From Non_duplicate_Rows_table and again did all the steps -> \n",
        " - Text Preprocessing and gave high weitage to title by repeating it 3 times.\n",
        "- Took a first 500 frequent tags that cover the 90% of questions.\n",
        "- Now apply a logistic regression with tfidf vectorizer.\n",
        "- Now at last i applied  2 modles logistic regression and linear svm One vs rest classifier with hyperparameter tuning on BOW vectorizer.\n",
        "- Compare all models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnsnqTaSM71X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}